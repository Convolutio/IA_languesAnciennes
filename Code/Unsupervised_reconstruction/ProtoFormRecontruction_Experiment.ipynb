{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "9N66w8WAeYxN"
   },
   "source": [
    "# Neural unsupervised approach for proto-form reconstruction : experiment about the impact of the language model.\n",
    "\n",
    "Authors : Benjamin BADOUAILLE, Eliott CAMOU, Thomas HORRUT (Université de Bordeaux, CPBx)\n",
    "\n",
    "Supervised by : Rachel Bawden (Inria, Paris, ALMAnaCH group)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "This notebook contains an implementation of the proto-form reconstruction neural model that [Andre He et al.](https://arxiv.org/abs/2211.08684) designed. This implementation covers the training, the inference and the evaluation of the general model, with also the training of several language models of the proto-language. Some details and tests about generation, sampling and maximisation steps could be addressed in other [notebooks](./notebooks/).\n",
    "\n",
    "The aim of the experiment is to reproduce the latin reconstruction as in the authors' paper, with similar experimental conditions. The main change is that the training and the evaluation will be carried out several times with differently-configured prior language models. Then, the models' performances could be compared in function of these language models.\n",
    "\n",
    "*This experiment is led in the context of an end-of-preparatory-cycle scientific research project, whose analysis will fed a wider reflexion about AI potential and limitations for solving Historical Linguistics problems.*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "-MvHc45wJY77"
   },
   "source": [
    "# Framework's datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "XeoAJmIFJk8k"
   },
   "source": [
    "\n",
    "\n",
    "*   $L$ is the roman languages set with which we work (`french`, `romanian`, `spanish`, `italian`, `portuguese`).\n",
    "*   $\\Sigma$ is the IPA characters set which is used in our tokenisation. With the special characters `'('` and `')'`, it constitutes the input vocabulary for the language and edit models. The insertion edit model ($q_\\textrm{ins}$) returns a probability distribution over $\\Sigma \\cup \\{\\textrm{\"<del>\"}\\}$ and the substition one ($q_\\textrm{sub}$) returns over $\\Sigma \\cup \\{\\textrm{\"<del>\"}\\}$\n",
    "*   Let $C$ be the cognates pairs set. We note the batch size $B := |C|$. \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "ww6npXCfqHoZ"
   },
   "source": [
    "## $Σ$ initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Sd5Bt-NOhBY7",
    "outputId": "037d6172-74cc-4474-9dc9-f4d001049347"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IPA characters : \n",
      "/ z / / ɣ / / m / / ɒ / / u / / s / / ʲ / / ˈ / / ː / / ʔ / / ɨ / / o / / k / / p / / e / / a / / β / / ø / / b / / f / \n",
      "/ ɡ / / ʒ / / y / / ɲ / / ɾ / / ˌ / / ɛ / / d / / ɹ / / w / / x / / n / / l / / r / / œ / / ɐ / / ʁ / / v / / ʌ / / ʊ / \n",
      "/ ŋ / / ʝ / / ʰ / / ʎ / / j / / h / / ð / / ʃ / / ɪ / / - / / ɔ / / ̃ / / ə / / ɑ / / i / / θ / / t / / ɥ / \n",
      "Length of the vocabulary: 58\n"
     ]
    }
   ],
   "source": [
    "from data import vocab\n",
    "\n",
    "IPA_VOC = list(vocab.SIGMA.keys())\n",
    "\n",
    "print(\"IPA characters : \")\n",
    "for i in range(len(IPA_VOC)):\n",
    "    print(f'/ {IPA_VOC[i]} /', end = ' ')\n",
    "    if i%20==19: print()\n",
    "print('\\nLength of the vocabulary:', len(IPA_VOC))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing of $C$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.getDataset import getCognatesSet\n",
    "from data.vocab import computeInferenceData\n",
    "from models.articleModels import ModernLanguages\n",
    "from models.models import InferenceData\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "cognates:dict[ModernLanguages, InferenceData] = {lang:computeInferenceData(pad_sequence(lst, batch_first=True)) for (lang, lst) in getCognatesSet().items()}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Edit models initialisation\n",
    "\n",
    "For each iteration of the Bouchard-Côté et al.' probabilistic reconstruction model, the backward dynamic program is run and the edit models are trained from the computed probabilities. This initialisation step is independant of the prior language model choice and the initial state of the edit models after these iterations is therefore the same for each of the future trainings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Source.editModel import EditModel\n",
    "\n",
    "INPUT_DIMENSION, HIDDEN_DIMENSION = 50, 50 # choice described in the appendix A.4 of the article\n",
    "edit_models = {lang:EditModel(cognates[lang], lang, ) for lang in cognates}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.getDataset import getIteration\n",
    "\n",
    "for bouchardIteration in (1,2,3,4):\n",
    "    samples = computeInferenceData(pad_sequence(getIteration(bouchardIteration)))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
