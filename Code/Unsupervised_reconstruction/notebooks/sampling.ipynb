{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import getcwd, chdir\n",
    "\n",
    "if getcwd().endswith('notebooks'):\n",
    "    chdir('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform the cognates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the generation, the cognates already need to be transformed into the ByteTensor format. Each ByteTensor in the list of dictionnaries has the elementary shape $\\left( |y_{c, l}|\\right)$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import tensor, Tensor, uint8\n",
    "from data.vocab import vocabulary\n",
    "from data.getDataset import getCognatesSet\n",
    "from Source.utils import dl_to_ld\n",
    "from models.types import ModernLanguages\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "cognates: list[dict[ModernLanguages, Tensor]] = [{lang:tensor(data=vocabulary(list(d[lang])), dtype=uint8, device=device) for lang in d} for d in dl_to_ld(getCognatesSet())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Samples generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Choose one of the two proposed generation methods for the test by setting the constant below.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERATE_WITH_ALGORITHM = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLES_NUMBER_PER_COGNATE_GROUP = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples: list[Tensor] = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Random prototype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Tests.createSamples import createSamplesBatch\n",
    "if not GENERATE_WITH_ALGORITHM:\n",
    "    samples = createSamplesBatch(len(cognates), SAMPLES_NUMBER_PER_COGNATE_GROUP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. With the generation algorithm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose the first Bouchard-Côté model's iteration for the reconstructions from which the generation will be processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.getDataset import getIteration\n",
    "from Source.generateProposals import generateProposalsFromCurrentReconstructions\n",
    "\n",
    "if GENERATE_WITH_ALGORITHM:\n",
    "    currentReconstructions: list[Tensor] = [tensor(data=vocabulary(list(word)), dtype=uint8, device=device) for word in getIteration(1)]\n",
    "    samples = generateProposalsFromCurrentReconstructions(currentReconstructions, cognates, SAMPLES_NUMBER_PER_COGNATE_GROUP)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Source.reconstructionModel import ReconstructionModel\n",
    "from models.types import MODERN_LANGUAGES\n",
    "\n",
    "LSTM_INPUT_DIM = 50\n",
    "LSTM_HIDDEN_DIM = 50\n",
    "\n",
    "randomEditModel = ReconstructionModel(MODERN_LANGUAGES, vocabulary, LSTM_INPUT_DIM, LSTM_HIDDEN_DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "False Language Model with neutral probability for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.types import InferenceData_Samples\n",
    "from lm.PriorLM import PriorLM\n",
    "from torch import zeros, float32\n",
    "\n",
    "class LM(PriorLM):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def inference(self, reconstructions: InferenceData_Samples):\n",
    "        return zeros(size=reconstructions[1].size(), dtype=float32, device=device)\n",
    "    \n",
    "random_lm = LM()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute unnormalized probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset of roughly $\\frac{B \\cdot C}{b \\cdot c}$ mini batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.reconstruction_datasets import samplingDataLoader\n",
    "\n",
    "MINI_BATCH_SHAPE = (len(cognates)//8, 50)\n",
    "dataloader = samplingDataLoader(samples, cognates, MINI_BATCH_SHAPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The unnormalized probabilities are computed from by running the inference in the prior language model and the forward dynamic program for each edit model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Example:</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pour une itération : 2.4593262672424316s ; Au total sur le dataset : 0.0h13.0m\n",
      "Probs tensor shape from french's edit model: torch.Size([401, 50])\n",
      "Probs tensor shape from spanish's edit model: torch.Size([401, 50])\n",
      "Probs tensor shape from italian's edit model: torch.Size([401, 50])\n",
      "Probs tensor shape from portuguese's edit model: torch.Size([401, 50])\n",
      "Probs tensor shape from romanian's edit model: torch.Size([401, 50])\n",
      "Probs tensor shape from prior edit model: torch.Size([401, 50])\n",
      "Unnormalized probs tensor shape: torch.Size([401, 50])\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "t1 = time()\n",
    "elt = next(iter(dataloader))\n",
    "edit_models_results = randomEditModel.forward_dynProg(*elt[0])\n",
    "prior_lm_results = random_lm.inference(elt[0][0])\n",
    "t2 = time()\n",
    "dt = t2 - t1\n",
    "total_time = dt*(SAMPLES_NUMBER_PER_COGNATE_GROUP*len(cognates))/(len(cognates)//8 * 50)\n",
    "print(f\"Pour une itération : {dt}s ; Au total sur le dataset : {total_time//3600}h{total_time//60}m\")\n",
    "\n",
    "for lang in edit_models_results:\n",
    "    print(f'Probs tensor shape from {lang}\\'s edit model: {edit_models_results[lang].size()}')\n",
    "print(f'Probs tensor shape from prior edit model:', prior_lm_results.size())\n",
    "\n",
    "unnormalized_probs = prior_lm_results\n",
    "for lang in edit_models_results:\n",
    "    unnormalized_probs += edit_models_results[lang]\n",
    "print('Unnormalized probs tensor shape:', unnormalized_probs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Complete iteration:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.13 GiB (GPU 0; 11.76 GiB total capacity; 6.51 GiB already allocated; 2.11 GiB free; 8.46 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[0;32m----> 2\u001b[0m     __edit_models_results \u001b[38;5;241m=\u001b[39m \u001b[43mrandomEditModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_dynProg\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     __prior_lm_results \u001b[38;5;241m=\u001b[39m random_lm\u001b[38;5;241m.\u001b[39minference(data[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m      4\u001b[0m     __results \u001b[38;5;241m=\u001b[39m __prior_lm_results\n",
      "File \u001b[0;32m~/Documents/deported_run/Source/reconstructionModel.py:181\u001b[0m, in \u001b[0;36mReconstructionModel.forward_dynProg\u001b[0;34m(self, samples, cognates)\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m language \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlanguages:\n\u001b[1;32m    180\u001b[0m         model: EditModel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__editModels[language]  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m--> 181\u001b[0m         mutation_prob \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_mutation_prob\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msources\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcognates\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    183\u001b[0m         probs[language] \u001b[38;5;241m=\u001b[39m mutation_prob  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m probs\n",
      "File \u001b[0;32m~/Documents/deported_run/Source/dynamicPrograms.py:37\u001b[0m, in \u001b[0;36mcompute_mutation_prob\u001b[0;34m(model, sources_, targets_, return_posteriors)\u001b[0m\n\u001b[1;32m     34\u001b[0m max_source_sequenceLength \u001b[38;5;241m=\u001b[39m max_rawSource_sequenceLength \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     35\u001b[0m max_target_sequenceLength \u001b[38;5;241m=\u001b[39m max_rawTarget_sequenceLength \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m---> 37\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache_probs\u001b[49m\u001b[43m(\u001b[49m\u001b[43msources_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# apparently faster in numpy for indexing but\u001b[39;00m\n\u001b[1;32m     41\u001b[0m f_sub \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfull((max_source_sequenceLength, max_target_sequenceLength,\n\u001b[1;32m     42\u001b[0m                    \u001b[38;5;241m*\u001b[39mbatch_shape), fill_value\u001b[38;5;241m=\u001b[39mBIG_NEG, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "File \u001b[0;32m~/Documents/deported_run/Source/editModel.py:195\u001b[0m, in \u001b[0;36mEditModel.cache_probs\u001b[0;34m(self, sources, targets)\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat((torch\u001b[38;5;241m.\u001b[39mcat((t, torch\u001b[38;5;241m.\u001b[39mzeros((padding_x, \u001b[38;5;241m*\u001b[39mt\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m1\u001b[39m:]), device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m),\n\u001b[1;32m    191\u001b[0m                       torch\u001b[38;5;241m.\u001b[39mzeros((t\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, padding_y, \u001b[38;5;241m*\u001b[39mt\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m2\u001b[39m:]), device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;66;03m# dim = (|x|+2, |y|+2, c, b)\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__cachedProbs \u001b[38;5;241m=\u001b[39m {op: lengthen(t, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 195\u001b[0m                       \u001b[38;5;28;01mfor\u001b[39;00m (op, t) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_and_select\u001b[49m\u001b[43m(\u001b[49m\u001b[43msources\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitems()}\n",
      "File \u001b[0;32m~/Documents/deported_run/Source/editModel.py:144\u001b[0m, in \u001b[0;36mEditModel.forward_and_select\u001b[0;34m(self, sources, targets)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;124;03mRun the forward method of the model and select the probabilities of interest for the cognates provided in input. During the selection, the probabilities for undefined (x[i],y[:j]) input couples (i.e. with one of the two input containing at least one padding token) are neutralized.\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;124;03mReturns a dictionnary of the probabilities for each operations (ins, sub, end, dlt).\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \n\u001b[1;32m    139\u001b[0m \u001b[38;5;124;03mTensors shape = (|x|+1, |y|+1, c, b)\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvertForIndexing\u001b[39m(t): \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(\n\u001b[1;32m    142\u001b[0m     (t, torch\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m*\u001b[39mt\u001b[38;5;241m.\u001b[39msize()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m1\u001b[39m), device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 144\u001b[0m sub_results, ins_results \u001b[38;5;241m=\u001b[39m (convertForIndexing(t)\n\u001b[1;32m    145\u001b[0m                             \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m(sources, targets))\n\u001b[1;32m    146\u001b[0m x_l, y_l, c, b \u001b[38;5;241m=\u001b[39m sub_results\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# |x|+1, |y|+1, c, b\u001b[39;00m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;66;03m# neutralizes results for the padding and eos tokens\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/deported_run/Source/editModel.py:144\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;124;03mRun the forward method of the model and select the probabilities of interest for the cognates provided in input. During the selection, the probabilities for undefined (x[i],y[:j]) input couples (i.e. with one of the two input containing at least one padding token) are neutralized.\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;124;03mReturns a dictionnary of the probabilities for each operations (ins, sub, end, dlt).\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \n\u001b[1;32m    139\u001b[0m \u001b[38;5;124;03mTensors shape = (|x|+1, |y|+1, c, b)\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvertForIndexing\u001b[39m(t): \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(\n\u001b[1;32m    142\u001b[0m     (t, torch\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m*\u001b[39mt\u001b[38;5;241m.\u001b[39msize()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m1\u001b[39m), device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 144\u001b[0m sub_results, ins_results \u001b[38;5;241m=\u001b[39m (\u001b[43mconvertForIndexing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m                             \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m(sources, targets))\n\u001b[1;32m    146\u001b[0m x_l, y_l, c, b \u001b[38;5;241m=\u001b[39m sub_results\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# |x|+1, |y|+1, c, b\u001b[39;00m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;66;03m# neutralizes results for the padding and eos tokens\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/deported_run/Source/editModel.py:141\u001b[0m, in \u001b[0;36mEditModel.forward_and_select.<locals>.convertForIndexing\u001b[0;34m(t)\u001b[0m\n\u001b[0;32m--> 141\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvertForIndexing\u001b[39m(t): \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.13 GiB (GPU 0; 11.76 GiB total capacity; 6.51 GiB already allocated; 2.11 GiB free; 8.46 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "for data in dataloader:\n",
    "    __edit_models_results = randomEditModel.forward_dynProg(*data[0])\n",
    "    __prior_lm_results = random_lm.inference(data[0][0])\n",
    "    __results = __prior_lm_results\n",
    "    for lang in __edit_models_results:\n",
    "        __results += __edit_models_results[lang]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
