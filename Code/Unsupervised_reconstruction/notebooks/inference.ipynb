{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inferences in prior Language Models\n",
    "\n",
    "*Computing of $p(x)$*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import chdir, getcwd\n",
    "\n",
    "if getcwd().endswith('notebooks'):\n",
    "    chdir('..')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot data padding for $n$-grams\n",
    "\n",
    "* If $L < n$:\n",
    "\n",
    "If the sequence's length is lower than $n$ -- the order of the $n$-gram --, so the sequence is padded with closing boundaries `')'`.\\\\\n",
    "For instance, `'(b)'` string will be processed by the 4-gram as `'(b))'` so at least one inference can be done in the language model.\n",
    "\n",
    "In training, the probabilities of this kind of $n$-gram will be computed by lower-order $(n-k)$-gram models.\n",
    "\n",
    "* If $L < L_\\textrm{max}$:\n",
    "\n",
    "The inference in a batch of strings will force the algorithm to process $n$-grams containing padding empty characters. To neutralize their useless probabilities, the empty characters are represented with a \"full one-hot\" vector, containing only values equaling `1`. Then, the computed transition probabilities will be higher than 1 and we after limit it to 1 with the max operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"(ab)\"\n",
      "tensor([[0., 0., 0., 1., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.]])\n",
      "\"(abcb)\"\n",
      "tensor([[0., 0., 0., 1., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1.]])\n",
      "\"(cba)\"\n",
      "tensor([[0., 0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1.],\n",
      "        [1., 1., 1., 1., 1.]])\n",
      "\"(b)\"\n",
      "tensor([[0., 0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pack_sequence\n",
    "from torch.nn.functional import one_hot\n",
    "V = {'a':0, 'b':1, 'c':2, \"(\":3, ')':4}\n",
    "V_inv = ['a', 'b', 'c', \"(\", \")\"]\n",
    "raw_batch = ['(ab)', '(abcb)', '(cba)', '(b)']\n",
    "batch = pack_sequence([one_hot(torch.LongTensor([V[c] for c in w])) for w in raw_batch], enforce_sorted=False) # size = (L, B)\n",
    "\n",
    "from lm.PriorLM import NGramLM\n",
    "\n",
    "model = NGramLM([], 4)\n",
    "paddedData = torch.exp(model.padDataToNgram((batch, None, None)))\n",
    "for w in range(4):\n",
    "    print(f'\\\"{raw_batch[w]}\\\"')\n",
    "    print(paddedData[:,w,:]) # dim = (L, V)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
