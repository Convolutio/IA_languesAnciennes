\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{campbell2013historical,Hock+2021}
\citation{durham1969application,eastlack1977iberochange,lowe1994reconstruction,covington-1998-alignment-multiple,kondrak2002algorithms}
\citation{bouchard-etal-2007-probabilistic,NIPS2007_7ce3284b}
\citation{bouchard-cote-etal-2009-improved,doi:10.1073/pnas.1204678110}
\citation{greenhill2008austronesian}
\citation{doi:10.1073/pnas.1204678110}
\citation{bye2011dissimilation}
\citation{nevins2010locality}
\citation{sen2012reconstructing}
\citation{yip1987english}
\citation{mohanan1982lexical}
\citation{wena1998functional}
\citation{fisiak2011historical}
\citation{bouchard-etal-2007-probabilistic}
\citation{NIPS2007_7ce3284b,bouchard-cote-etal-2009-improved,doi:10.1073/pnas.1204678110}
\citation{bouchard-etal-2007-probabilistic}
\citation{kiparsky1965phonological}
\citation{bouchard-etal-2007-probabilistic,NIPS2007_7ce3284b,bouchard-cote-etal-2009-improved,doi:10.1073/pnas.1204678110}
\citation{bouchard-etal-2007-probabilistic}
\citation{NIPS2007_7ce3284b}
\citation{bouchard-cote-etal-2009-improved}
\citation{bouchard-cote-etal-2009-improved}
\citation{doi:10.1073/pnas.1204678110}
\citation{bouchard-etal-2007-probabilistic}
\citation{bouchard-cote-etal-2009-improved}
\citation{NIPS2008_1651cf0d}
\citation{bouchard-cote-etal-2009-improved}
\citation{bouchard-cote-etal-2009-improved}
\citation{bouchard-cote-etal-2009-improved}
\citation{bouchard-etal-2007-probabilistic}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:overview}{{1}{2}{Overview of our paper. (a) We model the evolution of word forms as a generative process which applies many character-level edits to the ancestral form, producing a distribution over the output word form $y$ and edit sequence $\Delta $. (b) Using a dynamic program, we can compute the distribution over output words, $p(y \mid x)$. We model this for every language branch $l \in L$. (c) Our method uses EM to infer ancestral forms. For the E-step, we want to sample from the posterior distribution, where $y$ is observed but $x$ is not. (f) With samples from the previous step fixed, we use another dynamic program to compute expected edit counts. (e) In the M-step, we use these edit counts to train our character-level edit models $q$, parameterized as recurrent neural networks. $q$ determines the edit probabilities in (c) and thus influences the next round of samples. (d) After several EM iterations, we take the maximum likelihood word forms as the final reconstructions.\relax }{figure.caption.1}{}}
\newlabel{sec:relatedwork}{{2}{2}{Related Work}{section.2}{}}
\citation{meloni-etal-2021-ab}
\citation{DBLP:journals/corr/AharoniG16}
\citation{bouchard-etal-2007-probabilistic,NIPS2007_7ce3284b}
\citation{meloni-etal-2021-ab}
\citation{dinu-ciobanu-2014-building}
\citation{meloni-etal-2021-ab}
\citation{bouchard-cote-etal-2009-improved}
\citation{bouchard-etal-2007-probabilistic}
\newlabel{sec:dataset}{{4}{3}{Dataset}{section.4}{}}
\newlabel{sec:model}{{5}{3}{Model}{section.5}{}}
\citation{bouchard-cote-etal-2009-improved}
\citation{DBLP:journals/corr/AharoniG16}
\newlabel{sec:editproc}{{5.1}{4}{Word-Level Edit Process}{subsection.5.1}{}}
\newlabel{fig:pseudocode}{{2}{4}{Pseudocode describing the generative process behind $p(y, \Delta \mid x)$. Each input character is potentially deleted or substituted, with zero or more characters inserted afterwards. The probabilities of edits are specified by the character-level edit models $q_{\text {sub}}$ and $q_{\text {ins}}$ (\ref {sec:editmodel}). Each edit in the list $\Delta $ is represented as a tuple $(\text {op}, \omega , x, i, y')$, where $\text {op} \in \{\text {sub}, \text {ins}\}$, $\omega \in \Sigma $, and $(x, i, y')$ make up the context of the edit.\relax }{figure.caption.2}{}}
\citation{NIPS2008_1651cf0d}
\newlabel{sec:editmodel}{{5.2}{5}{Character-Level Model}{subsection.5.2}{}}
\newlabel{sec:mstep}{{6.2}{5}{Maximization Step}{subsection.6.2}{}}
\citation{bouchard-cote-etal-2009-improved}
\citation{bouchard-etal-2007-probabilistic}
\citation{bouchard-cote-etal-2009-improved}
\citation{bouchard-cote-etal-2009-improved}
\newlabel{fig:arch}{{3}{6}{Architecture diagram of the character-level edit model, denoted $q_{\text {sub/ins}}(\omega \mid x, i, y')$. The distribution of outcomes is dependent on both the input string and output history. Here our model is shown predicting edits for \textit {\textipa {I}} when the input is \textit {\textipa {prEsIO}} and the current output is \textit {pess}. The model predicts substitutions if the input character \textit {\textipa {I}} has not produced any outputs yet; otherwise it predicts characters to insert. Note that deletion \text {<del>} and end-of-insertion \text {<end>} are special outcomes of substitution and insertion. \relax }{figure.caption.3}{}}
\newlabel{sec:underfitting}{{6.4}{6}{Underfitting the Model}{subsection.6.4}{}}
\citation{bouchard-etal-2007-probabilistic,NIPS2007_7ce3284b,bouchard-cote-etal-2009-improved}
\citation{bouchard-etal-2007-probabilistic}
\citation{bouchard-cote-etal-2009-improved}
\citation{bouchard-etal-2007-probabilistic,NIPS2007_7ce3284b}
\citation{bouchard-cote-etal-2009-improved,doi:10.1073/pnas.1204678110}
\citation{bouchard-cote-etal-2009-improved}
\citation{bouchard-cote-etal-2009-improved}
\citation{bouchard-cote-etal-2009-improved}
\citation{bouchard-cote-etal-2009-improved}
\newlabel{sec:experiments}{{7}{7}{Experiments}{section.7}{}}
\newlabel{sec:comparison}{{7.1}{7}{Comparison to Previous Models}{subsection.7.1}{}}
\newlabel{sec:contextablation}{{7.3}{7}{Ablation: Context Length}{subsection.7.3}{}}
\citation{campbell2013historical,Hock+2021}
\citation{greenhill2008austronesian}
\citation{bouchard-cote-etal-2009-improved,doi:10.1073/pnas.1204678110}
\citation{greenhill2008austronesian}
\citation{bouchard-cote-etal-2009-improved}
\citation{bouchard-etal-2007-probabilistic}
\newlabel{fig:results}{{4}{8}{(Left) Our method significantly outperforms the classical baseline from \citet {bouchard-cote-etal-2009-improved}. Although the improvement is only a 7\% reduction in terms of edit distance, we reduce the error rate by 70\% as much as the classical model did from an untrained baseline. (Middle) Reducing the number of epochs per maximization step underfits the model but results in better reconstructions in the long run. (Right) When the learning algorithm is well-regularized, conditioning edit probabilities on wider contexts results in more accurate reconstructions. \relax }{figure.caption.4}{}}
\bibdata{references}
\bibcite{DBLP:journals/corr/AharoniG16}{{1}{2016}{{Aharoni and Goldberg}}{{}}}
\bibcite{bouchard-cote-etal-2009-improved}{{2}{2009}{{Bouchard-C\^{o}t\'{e} et~al.}}{{Bouchard-C\^{o}t\'{e}, Griffiths, and Klein}}}
\bibcite{doi:10.1073/pnas.1204678110}{{3}{2013}{{Bouchard-C\^{o}t\'{e} et~al.}}{{Bouchard-C\^{o}t\'{e}, Hall, Griffiths, and Klein}}}
\bibcite{NIPS2008_1651cf0d}{{4}{2008}{{Bouchard-C\^{o}t\'{e} et~al.}}{{Bouchard-C\^{o}t\'{e}, Klein, and Jordan}}}
\bibcite{bouchard-etal-2007-probabilistic}{{5}{2007{a}}{{Bouchard-C\^{o}t\'{e} et~al.}}{{Bouchard-C\^{o}t\'{e}, Liang, Griffiths, and Klein}}}
\bibcite{NIPS2007_7ce3284b}{{6}{2007{b}}{{Bouchard-C\^{o}t\'{e} et~al.}}{{Bouchard-C\^{o}t\'{e}, Liang, Klein, and Griffiths}}}
\bibcite{bye2011dissimilation}{{7}{2011}{{Bye}}{{}}}
\bibcite{campbell2013historical}{{8}{2013}{{Campbell}}{{}}}
\bibcite{covington-1998-alignment-multiple}{{9}{1998}{{Covington}}{{}}}
\bibcite{dinu-ciobanu-2014-building}{{10}{2014}{{Dinu and Ciobanu}}{{}}}
\bibcite{durham1969application}{{11}{1969}{{Durham and Rogers}}{{}}}
\bibcite{eastlack1977iberochange}{{12}{1977}{{Eastlack}}{{}}}
\bibcite{fisiak2011historical}{{13}{2011}{{Fisiak}}{{}}}
\bibcite{greenhill2008austronesian}{{14}{2008}{{Greenhill et~al.}}{{Greenhill, Blust, and Gray}}}
\bibcite{Hock+2021}{{15}{2021}{{Hock}}{{}}}
\bibcite{kiparsky1965phonological}{{16}{1965}{{Kiparsky}}{{}}}
\bibcite{kondrak2002algorithms}{{17}{2002}{{Kondrak}}{{}}}
\bibcite{lowe1994reconstruction}{{18}{1994}{{Lowe and Mazaudon}}{{}}}
\bibcite{meloni-etal-2021-ab}{{19}{2021}{{Meloni et~al.}}{{Meloni, Ravfogel, and Goldberg}}}
\bibcite{mohanan1982lexical}{{20}{1982}{{Mohanan}}{{}}}
\bibcite{nevins2010locality}{{21}{2010}{{Nevins}}{{}}}
\bibcite{sen2012reconstructing}{{22}{2012}{{Sen}}{{}}}
\bibcite{wena1998functional}{{23}{1998}{{Welna}}{{}}}
\bibcite{yip1987english}{{24}{1987}{{Yip}}{{}}}
\bibstyle{acl_natbib}
\newlabel{sec:appendix}{{A}{10}{Appendix}{appendix.A}{}}
\newlabel{appendix:fdp}{{A.2}{10}{Forward Dynamic Program}{subsection.A.2}{}}
\newlabel{appendix:bdp}{{A.3}{10}{Backward Dynamic Program}{subsection.A.3}{}}
\newlabel{ipa}{{1}{11}{IPA transcriptions for several cognate sets after our preprocessing steps, along with gold labels and example reconstructions from our best performing unsupervised reconstruction model\relax }{table.caption.7}{}}
\newlabel{fig:mineditpath}{{5}{11}{Example of possible proposals when the current reconstruction is \textit {\textipa {absEns}}, and it has a modern cognate \textit {\textipa {assEnte}}. We only show edit paths between the current sample and its Italian cognate here, but candidates can also be on paths between the current sample and its other modern cognates.\relax }{figure.caption.8}{}}
\gdef \@abspage@last{11}
