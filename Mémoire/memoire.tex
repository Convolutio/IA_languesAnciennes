\documentclass[12pt, french]{report}
%\usepackage[total={6.5in,10in}, top=0.8in, left=1in, includefoot]{geometry}
\usepackage[a4paper, total={6.5in, 10in}, top=0.8in, left=1in,
    headheight=48pt,
    includefoot, includehead]{geometry}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{mathptmx} %Times font (contraint par le CPBx)

\usepackage{babel, csquotes, xpatch}
% to install csquotes, xpatch and biblatex, download their zips on ctan.org
% and after (excepted for xpatch) copy the unzipped folders witht the .sty files
% in the <texmf-dist>/tex/latex/ repository (see tex-workshop logs to figure out
% the path of <texmf-dist>).
% For xpatch, read the README of its zip.
\usepackage[
    backend=biber,
    style=numeric
]{biblatex}
\addbibresource{biblio.bib}

\usepackage{amsfonts}
\usepackage{fancyhdr} % Pour la mise en page des en-têtes

\begin{document}
\pagestyle{fancy}
\fancyhead{}
\fancyhead[R]{\nouppercase{\hfill\leftmark}\\}
\fancyhead[L]{\nouppercase{\rightmark\hfill}}
\fancyfoot[L]{Mémoire CPBx}
\fancyfoot[R]{2023}
\begin{titlepage}
    \centering
    \vspace*{\fill}

    \huge\bfseries
    Les utilisations possibles de l'Intelligence Artificielle dans la linguistique historique
    
    \vspace*{1.5cm}
    \large 3 étudiants de CPBx
    
    \vspace*{\fill}
\end{titlepage}

\null
\newpage % Première page blanche pour la numérotation (contrainte de mise en page)
\section{Résumé}
\section{Abstract}
\section{Remerciements}

\tableofcontents
\section{Table des figures}
\section{Notations}

\chapter{Introduction}
\textit{Mise en contexte pour arriver à la problématique, quel est le potentiel de l'intelligence artificielle dans la linguistique historique ? ... ?}

\chapter{La linguistique historique et l'Intelligence Artificielle}
\section{La linguistique historique}
\subsection{Introduction à la linguistique historique}
\textit{Définir ce qu'est la linguistique historique, ce qu'elle étudie, et les mots de vocabulaires que nous allons rencontrer tout au long du mémoire.}
\subsection{Les différents principes}
\textit{Évidemment cette science repose sur des concepts, allant des propriétés synchoniques des mots aux à leurs aspects diachroniques.}
\subsection{Les atouts de l'Intelligence Artificielle dans ce domaine}
\textit{La linguistique historique fait face à de nombreux problèmes récurrents (traiter une grande quantité de textes pour l'homme, remarquer des motifs dans ces documents historiques). Alors que ce travail pourrait être effectué par une machine, grâce à sa capacité à traiter un grand nombres de données, et à chercher des similarités dans ces données. Avant, de voir les tâches où l'Intelligence Artificielle peut intervenir, il est d'abord nécessaire de voir en détail la conception des ces IA.}

Résoudre des problèmes de Linguistique Historique avec un ordinateur nécessite de lui faire traiter
du contenu textuel devant être abstrait sur des terrains parmi ceux de la \textbf{phonétique}, de la
\textbf{sémantique}, de la \textbf{morphologie} ou encore de la \textbf{syntaxe}.\\
\textbf{\textit{Développper un exemple pour illustrer ces 4 niveaux d'abstractions}}

La réalisation de ces abstractions s'inscrit dans le Traitement Automatisé du Langage Naturel (TAL),
un domaine à cheval entre la Linguistique et l'Informatique. L'Intelligence Artificielle y occupe
une place centrale pour sa capacité à effectuer des approximations améliorables avec de l'entraînement.


\section{L'IA dans le Traitement Automatisé du Langage Naturel}
\subsection{Introduction à l'apprentissage automatique}
\textit{Qu'est ce qu'une intelligence artificielle ?\\
    Qu'est ce qu'un réseau de neurones ?\\
    Quel est le principe derrière l'apprentissage automatique ?\\
    Définition des apprentissages supervisés/non supervisés
    Définition de propagation avant.
    Définition rétro-propagation du gradient.
    Exemple de FFNN pour tâche de classification}

De nombreux problèmes informatiques peuvent être résolus à travers la détermination
d'une fonction mathématique $f$ d'un $\mathbb{K}$-espace vectoriel $E$ vers un 
$\mathbb{K}$-espace $F$ (avec $\mathbb{K}$ correspondant à $\mathbb{R}$ ou
$\mathbb{C}$).\\
Ainsi, lorsqu'une fonction informatique conventionnelle effectue un traitement sur
une chaîne de caractères, une fonction $f$ a déjà été implicitement déterminée pour
réaliser la tâche.
La séquence de $n$ caractères encodés sous forme de bits forme un vecteur de l'espace
$\mathbb{R}^n$ et la chaîne renvoyée par $f$ est bien un élément d'un espace
$\mathbb{R}^{n'}$.

Il est aussi très fréquemment difficile -- voire impossible --
de poser une expression mathématique ou un algorithme pour répondre à certains 
problèmes. Dans ce cas là, $f$ est considérée comme hypothétique et on cherche à
l'approcher à partir d'un \textbf{modèle}, qu'on construit à partir des informations
qu'on dispose sur $f$, comme un ensemble de ses points 
$\{(x_k, y_k=f(x_k)), k \in S\}$.

\vspace{12pt}
Les \textbf{réseaux de neurones} sont d'excellents outils pour établir des modèles.
Mathématiquement, ce sont des compositions d'applications non-linéaires et linéaires
recevant un vecteur d'entrée représentant une donnée et sortant un vecteur de sortie
représentant un résultat dans un format cohérent avec le problème.\\
Le neurone artificiel le plus élémentaire effectue la \textbf{somme pondérée} des 
coefficients du vecteur d'entrée, puis calcule l'image de cette somme à travers
une \textbf{fonction d'activation}. La sortie du neurone est donc un réel ou un
complexe. Si on la note $y_j$, qu'on note $x$ le vecteur d'entrée dans $\mathbb{K}^n$,
$w_j$ le vecteur de \textbf{poids} associé au neurone et $\sigma$ sa fonction d'activation,
on a :
\begin{equation}
    y_j = \sigma(\sum_{i=0}^{n} w_{ij}x_i) = \sigma(<w_j, x>)
\end{equation}

\textit{introduire la matrice $W$ pour le calcul d'une couche\\
dire que les informations dans chaque coeff du vecteur x sont 
souvent abstraites et n'ont de sens que pour la machine (teaser vers sous-
section suivante) + leur importance est déterminée par les poids avec
l'entraînement\\ décrire l'entraînement (régression logistique+fct perte) \\
Exemple de la tâche de classification (connotation textuelle)\\
dire que $\sigma$ est un paramètre du réseau\\
on peut empiler plusieurs réseaux de neurones et plusieurs couches (paramètre
architecturale)}\\

\subsection{Traitement des donnnées}

Avant de donner à un réseau de neurones quelconque données récupérées pour son entraînement, il est nécessaire de préparer ces données. Le but étant de rendre ces données d'entrées correctes et compréhensibles pour notre intelligence artificielle. Cet préparation s'effectue suivant le type de tâche souhaité. Néanmoins, dans sa généralité, les étapes de préparation de données en TAL restent les mêmes.\\

% \footnote{À noter que nous parlons ici de « grandes » quantités de données à analyser, mais ce mot est relatif à un être humain, et encore, il arrive que ce nombre soit restreint, seulement, pour une IA c'est le contraire, nous parlerons plutôt de d'IA à « basses ressources » car ce nombre de données est très petits face à ce qu'elle peut gérer dans le monde moderne et la quantités de textes disponibles sur la toile.}
Tout d'abord, il faut normaliser (nettoyer) notre base de données qui peut être, par exemple, un corpus de textes, une liste de mots, provenant de la toile, ou d'un système de transcription audio-visuelle \footnote{Dans le cadre de ce mémoire, le but n'est pas de savoir comment parvient-on à extraire du texte à partir de ces sources, mais plutôt de savoir comment rendre ce texte compréhensible à une l'intelligence artificielle choisi.}
Dans ces données se trouvent évidemment des mots, avec des « caractéristiques » de la  langue \footnote{Pour simplifier, nous avons pris l'exemple d'une base de données  monolingue, mais il est possible qu'elle comporte plusieurs langues. Dans tout les cas, le procédé sera le même, si ce n'est qu'il faut s'adapter à chaque langue.} tel que les ponctuations, les lettres en capitales, les chiffres, ou bien les caractères spéciaux (comme le dièse que l'on retrouve souvent dans les \textit{tweets}). Seulement, il arrive parfois que certains de ces éléments rajoutent inutilement de la complexité pour une machine, sans que cela apporte plus de sens, ou bien sont non-désirées (voir incorrectes), ainsi on décide de les supprimer (ou les remplacer) pour ne garder que ce qui nous intéresse. [Exemple]\\

%[lib nltk]
Après avoir nettoyé notre base de données, il faut segmenter notre texte en mots ou sous-mots, autrement dit, il faut tokeniser notre texte. La tokenisation correspond à la segmentation de  chaînes de caractères (comme notre texte) en tokens (mots, sous-mots, ponctuations).
% Nous pourrions discuter longuement de ce qui devrait être pris comme tokens ou non. Par exemple, si nous devons prendre en compte la ponctuation en tant que token. Mais aussi les clitiques si elles doivent être pris en compte, ou si elles doivent être développées. Pour cela, nous vous renvoyons au Chapitre 2 de « Speech and Language Processing » de Jurasky.
Une approche naïve est de tokeniser notre texte en mots suivant les espaces
% Il est intéressant d’évoquer que suivant les différents niveaux de tokenisations (partant du caractères, au sous-mots, jusqu’au mots entier) aide à mieux respectivement à comprendre la morphologie, la sémantique (suivant les sous-mots des algorithmes statistiques), et la syntaxe (suivant les sous-mots basé sur les règles). Et il a été montré qu’il était suffisant de segmenter des phrases en sous-mots pour analyser le langage de manière pertinente. (Thèse Fourrier)
\footnote{Vous remarquerez déjà que ce processus ne s'applique pas au langue comme le Japonais, ou le Chinois.}. Prenons un exemple « J'aime les bananes. » tokenisé cela donne [« J'aime », « les », « bananes. »]. Cependant, cette approche pose des problèmes, en commençant par le token « bananes. » qui a la même signification avec ou sans point, mais qui sera considéré comme différent pour une IA. Nous pourrions ajouter la séparation suivant la ponctuation, mais alors nous obtiendrons pour « J'aime » les tokens [« J », « ' », « aime »] qui est une forme tout aussi problématique. Et il existe encore de nombreux cas (les abréviations, les points de suspension, etc.) où ce type de tokenisation pose problème. Une approche alternative est de tokeniser suivant des règles définis (par exemple, de prendre en compte les contractions comme « J'aime » et de le transformer en deux tokens [« Je », « aime »]), qui est une méthode beaucoup plus efficace que la première approche, mais montrera des limites face à des situations (ou mots) rares, ou alors il faudrait spécifier de nouvelles règles pour gérer ces cas. Ainsi, la solution proposée est une approche statistique, consistant à décomposer de plus en plus un mot en sous-mots \footnote{Remarquez que le terme \textit{mot} a un sens différent que celui qui le précède.} au fil que sa fréquence diminue. [Exemple]. Les quatres algorithmes de tokenisation en sous-mots les plus utilisées sont le \textit{Byte-Pair Encoding} (Sennrich et al., 2016), l'\textit{unigram language modeling} (Kudo, 2018), le \textit{WordPiece} (Schuster et Nakajima, 2012), et le \textit{SentencePiece}\footnote{Cet algorithme est une implémentation des deux premiers.} (Kudo et Richardson, 2018).\\

Pour la tâche de classification, vous avez vu que les mots d'entrées, [exemple], étaient convertis en une liste de nombre, autrement dit un vecteur, sur lequel il a été effectué des calculs afin d'obtenir un résultat (un nouveau vecteur), [exemple]. Une machine, un réseau de neurones, ne comprend que des nombres et ne sait procéder qu'à des calculs. Il existe différentes façons de convertir une chaîne de caractères (mots, tokens) en un vecteur, mais on retiendra deux méthodes l'encodage 1 parmi n et le plongement lexical (respectivement et plus communément appelés en anglais le \textit{one-hot encoding} et le \textit{word embedding}).\\

Discuter rapidement du one hot encoding, puis du word embedding (statique et contextuel)\\
    
\textit{Data splitting and batching}


\subsection{Architectures neuronales utiles au TAL}
\textit{Quels sont les différents outils ?}
\textit{Suivant, comment les parties précédentes ont été traités, ou comment les parties futures seront discutées, cette partie pourrait ne pas être nécessaire. Sinon, elle regroupera l'idée de comment on passe de notre langue naturelle à celle de la machine, de passer aux mots à des vecteurs ? Quels traitements théoriques (théoriques pour ce distinguer de la pratique dans la partie future) doient être effectués sur les mots ? En fait cette partie fait référence aux chapitres 2 et 6 de Jurasky. Voir même le chapitre 9, en supprimant la sous partie précendente pour pouvoir parler directement des réseaux de neurones appliqués à la linguistique, en d'autres termes, des réseaux récurrents, des modèles séquentielles (encodeurs-décodeurs) avec l'attention, et des Transformers.}\\

\subsubsection{Réseaux de neurones récurrents}

...

\subsubsection{Transformeurs}

...

\chapter{Les contributions de l'IA dans la linguistique historique}
\textit{C'est la partie 'Related Work', elle discute des différents aspects où la linguistique historique s'applique, à travers différents modèles.}
\section{Restoration de documents anciens}
\section{Déchiffrement de langues anciennes}

\chapter{Étude du cas de l'application de l'IA pour la reconstruction des proto-formes d'une langue}
\textit{Ici, on se place dans un cas concret, pour montrer que ce n'est pas que de la théorie. En proposant une expérience.}
\section{État de l'art}
\subsection{Conceptualisation du problème}
\textit{Définir clairement le problème du titre, énoncer et justifier le choix de notre modèle réseaux de neurones et des différents outils appliqués. Voir s'il est possible de faire apparaitre plusieurs démarches, c'est à dire, une approche statistique et une approche neuronale (toujours pour renforcer et montrer le potentiel de l'IA).}

\subsection{Dernières solutions neuronales}
\textit{Solution supervisée + non supervisée}

\subsection{Limites d'applicabilité}
\textit{Expliquer en quoi le non-supervisé donne plus d'espoir que le supervisé mais en quoi même cette approche présente des limites.}\\
\textit{Transition avec la problématique de l'article scientifique}\\

\section{Observation expériementale d'une limite d'applicabilité d'une approche}
\subsection{Méthode}
\subsection{Récupération de la base de données}
\textit{Il est fort possible que cette partie se regroupe avec la partie suivante, car il n'y aura pas grand chose à dire.}

\subsection{Normaliser les données}

\subsection{Analyse}
\subsection{Critiques}
\textit{Il reste ici quelques sous parties à détailler.}

\chapter{Conclusion}
\section{Synthèse}
\textit{Résume tout ce qui a été dit.}
\section{Les différentes limites posées aujourd'hui}
\textit{une partie des limites aura déjà été traitée dans le chapitre précédent. Cette sous partie se veut résumer ces limites, et aller dans les limites générales (voir acutelles) de l'IA dans  la linguisitique historique.}
\section{Les perspectives de l'IA dans la linguistique historique}
\textit{Ouverture, dépassement de certaines limites, évolution des modèles.}

\chapter{Références}
\section{Bibliographie}

\printbibliography

\end{document}