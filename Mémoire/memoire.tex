\documentclass[12pt, french]{report}
%\usepackage[total={6.5in,10in}, top=0.8in, left=1in, includefoot]{geometry}
\usepackage[a4paper, total={6.5in, 10in}, top=0.8in, left=1in,
    headheight=48pt,
    includefoot, includehead]{geometry}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{mathptmx} %Times font (contraint par le CPBx)
\usepackage{amsmath} % for matrices print

%glossaire auto, on peut s'y référer dans le latex avec \Gls{maRef}
\usepackage{glossaries}

\usepackage{babel, csquotes, xpatch}
% to install csquotes, xpatch and biblatex, download their zips on ctan.org
% and after (excepted for xpatch) copy the unzipped folders witht the .sty files
% in the <texmf-dist>/tex/latex/ repository (see tex-workshop logs to figure out
% the path of <texmf-dist>).
% For xpatch, read the README of its zip.
\usepackage[
    backend=biber,
    style=numeric
]{biblatex}
\addbibresource{biblio.bib}

\usepackage{amsfonts}
\usepackage{fancyhdr} % Pour la mise en page des en-têtess

\begin{document}
\pagestyle{fancy}
\fancyhead{}
\fancyhead[R]{\nouppercase{\hfill\leftmark}\\}
\fancyhead[L]{\nouppercase{\rightmark\hfill}}
\fancyfoot[L]{Mémoire CPBx}
\fancyfoot[R]{2023}
\begin{titlepage}
    \centering
    \vspace*{\fill}

    \huge\bfseries
    Les utilisations possibles de l'Intelligence Artificielle dans la linguistique historique
    
    \vspace*{1.5cm}
    \large 3 étudiants de CPBx
    
    \vspace*{\fill}
\end{titlepage}

\null
\thispagestyle{empty}
\newpage % Première page blanche pour la numérotation (contrainte de mise en page)
\section{Résumé}
\section{Abstract}
\section{Remerciements}

\tableofcontents
\listoffigures
\listoftables %si inutile, on le virera
\makeglossary

%ici c'est un exemple
\newglossaryentry{formula}
{
        name=formula,
        description={A mathematical expression}
}

\newacronym{gcd}{GCD}{Greatest Common Divisor}

\newacronym{lcm}{LCM}{Least Common Multiple}

\section{Table des figures}
\section{Notations}

\chapter{Introduction}
\textit{Mise en contexte pour arriver à la problématique, quel est le potentiel de l'intelligence artificielle dans la linguistique historique ? ... ?}

\chapter{La linguistique historique et l'Intelligence Artificielle}
\section{La linguistique historique}
\subsection{Introduction à la linguistique historique}
\textit{Définir ce qu'est la linguistique historique, ce qu'elle étudie, et les mots de vocabulaires que nous allons rencontrer tout au long du mémoire.}
\subsection{Les différents principes}
\textit{Évidemment cette science repose sur des concepts, allant des propriétés synchoniques des mots aux à leurs aspects diachroniques.}
\subsection{Les atouts de l'Intelligence Artificielle dans ce domaine}
\textit{La linguistique historique fait face à de nombreux problèmes récurrents (traiter une grande quantité de textes pour l'homme, remarquer des motifs dans ces documents historiques). Alors que ce travail pourrait être effectué par une machine, grâce à sa capacité à traiter un grand nombres de données, et à chercher des similarités dans ces données. Avant, de voir les tâches où l'Intelligence Artificielle peut intervenir, il est d'abord nécessaire de voir en détail la conception des ces IA.}

Résoudre des problèmes de Linguistique Historique avec un ordinateur nécessite de lui faire traiter
du contenu textuel devant être abstrait sur des terrains parmi ceux de la \textbf{phonétique}, de la
\textbf{sémantique}, de la \textbf{morphologie} ou encore de la \textbf{syntaxe}.\\
\textbf{\textit{Développper un exemple pour illustrer ces 4 niveaux d'abstractions}}

La réalisation de ces abstractions s'inscrit dans le Traitement Automatisé du Langage Naturel (TAL),
un domaine à cheval entre la Linguistique et l'Informatique. L'Intelligence Artificielle y occupe
une place centrale pour sa capacité à effectuer des approximations améliorables avec de l'entraînement.


\section{L'IA dans le Traitement Automatisé du Langage Naturel}
\subsection{Introduction à l'apprentissage automatique}
\textit{Qu'est ce qu'une intelligence artificielle ?\\
    Qu'est ce qu'un réseau de neurones ?\\
    Quel est le principe derrière l'apprentissage automatique ?\\
    Définition des apprentissages supervisés/non supervisés
    Définition de propagation avant.
    Définition rétro-propagation du gradient.
    Exemple de FFNN pour tâche de classification}

Un important nombre de problèmes informatiques peut être résolu à travers la détermination d'une fonction mathématique $f$ d'un espace vectoriel $\mathbb{K}^n$ vers un espace vectoriel $\mathbb{K}^{n'}$ (avec $\mathbb{K}$ correspondant à $\mathbb{R}$ ou $\mathbb{C}$).\\
Lorsqu'un algorithme conventionnel est développé pour réaliser un tâche, $f$ a déjà implicitement été trouvé. Par exemple, derrière un traitement opéré sur une chaîne de caractères, elle existe bien, avec pour entrée une séquence de $n$ caractères encodés sous forme de bits qui forme un vecteur de l'espace $\mathbb{R}^n$ et pour sortie un élément d'un espace $\mathbb{R}^{n'}$ représentant la chaîne de sortie.\\
En revanche, de nombreux cas demeurent où il est difficile -- voire impossible -- de poser une expression mathématique ou un algorithme pour répondre à certains problèmes. On considère alors $f$ comme hypothétique et on cherche à l'approcher à partir d'un \textbf{modèle}, qu'on construit à partir des informations qu'on dispose sur $f$, comme un ensemble de ses points $\{(x_k, y_k=f(x_k)), k \in S\}$, à travers une tâche dite de \textbf{régression}.

\vspace{12pt}
Les \textbf{réseaux de neurones} sont d'excellents outils pour établir des modèles.
Mathématiquement, ce sont des compositions d'applications non-linéaires et linéaires
recevant un vecteur d'entrée représentant une donnée et sortant un vecteur de sortie
représentant un résultat dans un format cohérent avec le problème.\\
Le neurone artificiel le plus élémentaire effectue la \textbf{somme pondérée} des 
coefficients du vecteur d'entrée, puis calcule l'image de cette somme à travers
une fonction non-linéaire dite \textbf{d'activation}. La sortie du neurone est donc un réel ou un
complexe. Si on la note $y_i$, qu'on note $x$ le vecteur d'entrée dans $\mathbb{K}^n$,
$w_i$ le vecteur de \textbf{poids} associé au neurone et $\sigma$ sa fonction d'activation,
on a :
\begin{equation}
    y_i = \sigma(\sum_{j=0}^{n} w_{ij}x_j) = \sigma(<w_i, x>)
\end{equation}

Une \textbf{couche de neurones} est la mise en commun d'un nombre abritraire $N$ de neurones devant prédire des sorties $y_i$ différentes. Leurs vecteurs de poids $w_i$ diffèreront donc. En revanche, leur fonction d'activation est identique. La sortie d'une couche est donc un vecteur $y$ pouvant s'écrire comme dans l'équation \ref{def_couche}.

\begin{equation} \label{def_couche}
    \begin{split}
        y & =
        \left(\begin{matrix}
            \sigma(<w_1, x>) \\
            \sigma(<w_2, x>) \\
            \vdots \\
            \sigma(<w_i, x>) \\
            \vdots \\
            \sigma(<w_N, x>)
        \end{matrix}\right)
        =
        \sigma\left(\begin{matrix}
            \sum_{j=0}^{n} w_{1j}x_j \\
            \sum_{j=0}^{n} w_{2j}x_j \\
            \vdots \\
            \sum_{j=0}^{n} w_{ij}x_j \\
            \vdots \\
            \sum_{j=0}^{n} w_{Nj}x_j
        \end{matrix}\right)
        = \sigma\left(
            \left(\begin{matrix}
                w_{11} & w_{12} & \hdots & w_{1j} & \hdots & w_{1n} \\
                w_{21} & w_{22} & \hdots & w_{2j} & \hdots & w_{2n} \\
                \vdots & \vdots & \ddots & \vdots & \ddots & \vdots \\
                w_{i1} & w_{i2} & \hdots & w_{ij} & \hdots & w_{in} \\
                \vdots & \vdots & \ddots & \vdots & \ddots & \vdots \\
                w_{N1} & w_{N2} & \hdots & w_{Nj} & \hdots & w_{Nn} \\
            \end{matrix}\right)
            \left(\begin{matrix}
                x_1 \\ x_2 \\ \vdots \\ x_j \\ \vdots \\ x_n
            \end{matrix}\right)
        \right) \\
        & = \sigma(Wx)
    \end{split}
\end{equation}

Un réseau neuronal est ainsi formé à partir de la mobilisation d'une ou plusieurs couches. L'intuition derrière l'utilisation de couches intermédiaires est que la machine puisse être capable d'apprendre à construire des \textbf{représentations adéquates} des données pour effecuter la prédiction finalement voulue avec pertinence. On parle alors d'\textbf{apprentissage profond} et cette technique offre des réponses face aux difficultés d'abstraction soulevées par les problèmes de TAL.\cite{jurafsky_ffnn}


\textit{dire que les informations dans chaque coeff du vecteur x sont 
souvent abstraites et n'ont de sens que pour la machine (teaser vers sous-
section suivante) + leur importance est déterminée par les poids avec
l'entraînement\\ décrire l'entraînement (régression logistique+fct perte) \\
Exemple de la tâche de classification (connotation textuelle)\\
dire que $\sigma$ est un paramètre du réseau\\
on peut empiler plusieurs réseaux de neurones et plusieurs couches (paramètre
architecturale)}\\

\subsection{Traitement des donnnées}

Avant de donner à un réseau de neurones quelconque données récupérées pour son entraînement, il est nécessaire de préparer ces données. Le but étant de rendre ces données d'entrées correctes et compréhensibles pour notre intelligence artificielle. Cet préparation s'effectue suivant le type de tâche souhaité. Néanmoins, dans sa généralité, les étapes de préparation de données en TAL restent les mêmes.\\

% \footnote{À noter que nous parlons ici de \og grandes \fg quantités de données à analyser, mais ce mot est relatif à un être humain, et encore, il arrive que ce nombre soit restreint, seulement, pour une IA c'est le contraire, nous parlerons plutôt de d'IA à \og basses ressources \fg car ce nombre de données est très petits face à ce qu'elle peut gérer dans le monde moderne et la quantités de textes disponibles sur la toile.}
Tout d'abord, il faut normaliser (nettoyer) notre base de données qui peut être, par exemple, un corpus de textes, une liste de mots, provenant de la toile, ou d'un système de transcription audio-visuelle \footnote{Dans le cadre de ce mémoire, le but n'est pas de savoir comment parvient-on à extraire du texte à partir de ces sources, mais plutôt de savoir comment rendre ce texte compréhensible à une l'intelligence artificielle choisie.}
Dans ces données se trouvent évidemment des mots, avec des \og caractéristiques \fg de la  langue \footnote{Pour simplifier, nous avons pris l'exemple d'une base de données  monolingue, mais il est possible qu'elle comporte plusieurs langues. Dans tout les cas, le procédé sera le même, si ce n'est qu'il faut s'adapter à chaque langue.} tel que les ponctuations, les lettres en capitales, les chiffres, ou bien les caractères spéciaux (comme le dièse que l'on retrouve souvent dans les \textit{tweets}). Seulement, il arrive parfois que certains de ces éléments rajoutent inutilement de la complexité pour une machine, sans que cela apporte plus de sens, ou bien sont non-désirées (voir incorrectes), ainsi on décide de les supprimer (ou les remplacer) pour ne garder que ce qui nous intéresse. [Exemple]\\

%[lib nltk]
Après avoir nettoyé notre base de données, il faut segmenter notre texte en mots ou sous-mots, autrement dit, il faut tokeniser notre texte. La tokenisation correspond à la segmentation de  chaînes de caractères (comme notre texte) en tokens (mots, sous-mots, ponctuations).
% Nous pourrions discuter longuement de ce qui devrait être pris comme tokens ou non. Par exemple, si nous devons prendre en compte la ponctuation en tant que token. Mais aussi les clitiques si elles doivent être pris en compte, ou si elles doivent être développées. Pour cela, nous vous renvoyons au Chapitre 2 de \og Speech and Language Processing \fg de Jurasky.
Une approche naïve est de tokeniser notre texte en mots suivant les espaces
% Il est intéressant d’évoquer que suivant les différents niveaux de tokenisations (partant du caractères, au sous-mots, jusqu’au mots entier) aide à mieux respectivement à comprendre la morphologie, la sémantique (suivant les sous-mots des algorithmes statistiques), et la syntaxe (suivant les sous-mots basé sur les règles). Et il a été montré qu’il était suffisant de segmenter des phrases en sous-mots pour analyser le langage de manière pertinente. (Thèse Fourrier)
\footnote{Vous remarquerez déjà que ce processus ne s'applique pas au langue comme le Japonais, ou le Chinois.}. Prenons un exemple \og J'aime les bananes. \fg tokenisé cela donne [\og J'aime \fg, \og les \fg, \og bananes. \fg]. Cependant, cette approche pose des problèmes, en commençant par le token \og bananes. \fg qui a la même signification avec ou sans point, mais qui sera considéré comme différent pour une IA. Nous pourrions ajouter la séparation suivant la ponctuation, mais alors nous obtiendrons pour \og J'aime \fg les tokens [\og J \fg, \og ' \fg, \og aime \fg] qui est une forme tout aussi problématique. Et il existe encore de nombreux cas (les abréviations, les points de suspension, etc.) où ce type de tokenisation pose problème. Une approche alternative est de tokeniser suivant des règles définis (par exemple, de prendre en compte les contractions comme \og J'aime \fg et de le transformer en deux tokens [\og Je \fg, \og aime \fg]), qui est une méthode beaucoup plus efficace que la première approche, mais montrera des limites face à des situations (ou mots) rares, ou alors il faudrait spécifier de nouvelles règles pour gérer ces cas. Ainsi, la solution proposée est une approche statistique, consistant à décomposer de plus en plus un mot en sous-mots \footnote{Remarquez que le terme \textit{mot} a un sens différent que celui qui le précède.} au fil que sa fréquence diminue. [Exemple]. Les quatres algorithmes de tokenisation en sous-mots les plus utilisées sont le \textit{Byte-Pair Encoding} (Sennrich et al., 2016), l'\textit{unigram language modeling} (Kudo, 2018), le \textit{WordPiece} (Schuster et Nakajima, 2012), et le \textit{SentencePiece}\footnote{Cet algorithme est une implémentation des deux premiers.} (Kudo et Richardson, 2018).\\

Pour la tâche de classification, vous avez vu que les mots d'entrées, [exemple], étaient convertis en une liste de nombre, autrement dit un vecteur, sur lequel il a été effectué des calculs afin d'obtenir un résultat (un nouveau vecteur), [exemple]. Une machine, un réseau de neurones, ne comprend que des nombres et ne sait procéder qu'à des calculs. Il existe différentes façons de convertir une chaîne de caractères (mots, tokens) en un vecteur, mais on retiendra deux méthodes l'encodage 1 parmi n et le plongement lexical (respectivement et plus communément appelés en anglais le \textit{one-hot encoding} et le \textit{word embedding}).\\

Discuter rapidement du one hot encoding, puis du word embedding (statique et contextuel)\\

% Data splitting and batching


\subsection{Architectures neuronales utiles au TAL}
\textit{Quels sont les différents outils ?}
\textit{Suivant, comment les parties précédentes ont été traités, ou comment les parties futures seront discutées, cette partie pourrait ne pas être nécessaire. Sinon, elle regroupera l'idée de comment on passe de notre langue naturelle à celle de la machine, de passer aux mots à des vecteurs ? Quels traitements théoriques (théoriques pour ce distinguer de la pratique dans la partie future) doient être effectués sur les mots ? En fait cette partie fait référence aux chapitres 2 et 6 de Jurasky. Voir même le chapitre 9, en supprimant la sous partie précendente pour pouvoir parler directement des réseaux de neurones appliqués à la linguistique, en d'autres termes, des réseaux récurrents, des modèles séquentielles (encodeurs-décodeurs) avec l'attention, et des Transformers.}\\

\subsubsection{Réseaux de neurones récurrents}

...

% Transition vers les Transformeurs, discuter des problèmes que pose les RNN. 

\subsubsection{Transformeurs}

Précédemment, avec les RNN et les LTSM, nous avons introduit le mécanisme d'attention, permettant au réseau de se focaliser sur la manière dont les mots (éloignés) sont reliées les uns aux autres. Seulement, comme nous l'avons vu aussi, ces réseaux se basent sur des connexions récurrentes, rendant le calcul coûteux et la parallélisation difficile. Ainsi, pour pallier ce problème et gagner en performance, un nouveau modèle de réseau de neurones apparait sous le nom de \textit{Transformers} (Transformeurs en français) dans le papier \og Attention Is All You Need \fg\; de Vaswani et al. (2017). Ce modèle de type encodeur-décodeur se base sur l'attention multi-têtes, l'innovation majeure des \textit{Transformers}\footnote{Par simplification, nos efforts se concentrerons sur l'attention multi-têtes. Mais si nous devions détailler, un \textit{Transformers} se décompose en blocs de \textit{transformer}, dont chaque bloc contient une unité d'attention multi-têtes (masqué ou non) et un FFNN, accompagné de connexions résiduelles et des couches de normalisation. Pour plus de détails, nous vous invitons à regarder le papier de Vaswani et al. (2017).}.\\

L'attention multi-têtes permet d'étudier tous les vecteurs d'entrées, comme des mots, en même temps (de façon parallèle), et dont chaque tête qui la compose se focalise sur un aspect des \textit{interactions} entre les différents éléments de la séquence, [exemple].\\ 

Chaque tête contient un module d'auto attention (\textit{self-attention} en anglais) qui est utilisé pour permettre à chaque tokens $x_i$ de pouvoir \textit{intéragir} avec tous les autres tokens de la séquence $X$, en combinant trois vecteurs : un vecteur requête $q_i$, un vecteur clé $k_i$, un vecteur valeur $v_i$. Le vecteur requête (\textit{query}) correspondant au vecteur sur lequel on porte notre attention et qui sera comparé à tous les autres vecteurs, nommés les vecteurs de clés (\textit{keys}). Puis, nous avons le vecteur valeur(\textit{value}) qui sera utilisé pour représenter la \og valeur sémantique (du mot)\fg\; et sera multiplié par le poids d'attention calculé avec les autres vecteurs (requêtes et clés). [Figure : Exemple de la formation des différents vecteurs $q_i$, $k_i$, $v_i$ suivant une phrase X $[x_1, x_2, \dots, x_i, \dots, x_n]$]. Pour créer ces vecteurs, les vecteurs $x_i$ sont multipliés avec les matrices d'enchassement ($W^Q$, $W^K$, $W^V$)\footnote{Ces matrices sont obtenus à l'entrainement.} : 
\[ q_i = W^Q x_i \,;\; k_i = W^K x_i \,;\; v_i = W^V x_i\]

% Obtention des matrices Q, K, V respectivement l'ensemble des vecteurs $q_i$, $k_i$, $v_i$.

Ensuite, ... (calcule score d'attention)
\[SelfAttention(Q, K, V)\; =\; softmax(\frac{QK^T}{\sqrt{d_k}})V\]

%Puis on effectue les calculs d'attention en obtenant la matrice Z.
% Inspiré des notations de l’article de Jay Alammar : The illustrated transformer.

Enfin, ... (addition et concaténation)
% On concatène les matrices $Z_h$ (h étant [[1, nombre de tête]]) puis on les multiplie par une matrice de poids $W_O$ (également généré par l'entrainement) 

Il existe une variation, parfois, un masque est utilisé... (masque)
% Préciser qu'on peut limiter le "champ de vision" du modèle, [exemple, pour la génération on ne veut pas que le modèle compare/étudie les mots futures mais uniquement avec les précédents]  

Au tout début, nous avons évoqué que le modèle étudié tout les tokens en même temps (sans réccurence), mais alors comment le modèle connait la position de chaque élément de la séquence d'entrée ? Les RNN connaissaient la position des tokens de par leur structure, c'est à dire, de part le fait que chaque token passé séquentiellement dans le modèle. Contrairement, dans les Transformers nous devons indiquer la position de chaque token, la solution proposé dans le papier Vaswani et al. (2017) est d'encodé la position de chaque token par des fonctions sinus et cosinus\footnote{Nous invitons le lecteur à regarder le papier de Vaswani et al. (2017) pour se convaincre de la pertinence de ce choix d'encodage de position. Simplement, retenez que cela permet au modèle de s'entrainer avec la position relative des tokens, plutôt que la position absolue.}. Concrètement pour 

[Exemple]

\chapter{Les contributions de l'IA dans la linguistique historique}
\textit{Petite introduction avant de passer au papiers.}

\section{Restoration de documents anciens}
% Ithaca
...

\section{Déchiffrement de langues anciennes}
% MIT
Une autre tâche possible par l'intelligence artificielle est le déchiffrement de langue anciennes...

\chapter{Étude du cas de l'application de l'IA pour la reconstruction des proto-formes d'une langue}
\textit{Ici, on se place dans un cas concret, pour montrer que ce n'est pas que de la théorie. En proposant une expérience.}
\section{État de l'art}
\subsection{Conceptualisation du problème}
\textit{Définir clairement le problème du titre, énoncer et justifier le choix de notre modèle réseaux de neurones et des différents outils appliqués. Voir s'il est possible de faire apparaitre plusieurs démarches, c'est à dire, une approche statistique et une approche neuronale (toujours pour renforcer et montrer le potentiel de l'IA).}

\subsection{Dernières solutions neuronales}
\textit{Solution supervisée + non supervisée}

\subsection{Limites d'applicabilité}
\textit{Expliquer en quoi le non-supervisé donne plus d'espoir que le supervisé mais en quoi même cette approche présente des limites.}\\
\textit{Transition avec la problématique de l'article scientifique}\\

\section{Observation expériementale d'une limite d'applicabilité d'une approche}
\subsection{Méthode}
\subsection{Récupération de la base de données}
\textit{Il est fort possible que cette partie se regroupe avec la partie suivante, car il n'y aura pas grand chose à dire.}

\subsection{Normaliser les données}

\subsection{Analyse}
\subsection{Critiques}
\textit{Il reste ici quelques sous parties à détailler.}

\chapter{Conclusion}
\section{Synthèse}
\textit{Résume tout ce qui a été dit.}
\section{Les différentes limites posées aujourd'hui}
\textit{une partie des limites aura déjà été traitée dans le chapitre précédent. Cette sous partie se veut résumer ces limites, et aller dans les limites générales (voir acutelles) de l'IA dans  la linguisitique historique.}
\section{Les perspectives de l'IA dans la linguistique historique}
\textit{Ouverture, dépassement de certaines limites, évolution des modèles.}

\chapter{Références}
\section{Bibliographie}

\printbibliography

\end{document}