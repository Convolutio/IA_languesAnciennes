{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prior Language Models\n",
    "\n",
    "*Computing of $p(x)$*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this before continuing so that the imports work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import chdir, getcwd\n",
    "\n",
    "if getcwd().endswith('notebooks'):\n",
    "    chdir('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.vocab import vocabulary\n",
    "from models.types import EOS_TOKEN, SOS_TOKEN, PADDING_TOKEN\n",
    "\n",
    "print('|IPA| =', len(vocabulary)-3) # '-3' because `vocabulary` contains the IPA characters plus the special tokens listed below\n",
    "for token in (SOS_TOKEN, EOS_TOKEN, PADDING_TOKEN):\n",
    "    print(token, vocabulary[token])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.getDataset import getLMTrainingSet\n",
    "\n",
    "# Following 'Article Scientifique' there are three db of different sizes.\n",
    "DB_SIZE = [20_000, 10_000, 5_000]\n",
    "\n",
    "# Generate the three db of different sizes.\n",
    "tokens_20k, tokens_10k, tokens_5k = getLMTrainingSet(DB_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.backward_compatibility import worker_init_fn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.datasets import CC100\n",
    "\n",
    "dp = CC100(root='./out/cache', language_code='la')\n",
    "DataLoader(dp, shuffle=True, num_workers=4, worker_init_fn=worker_init_fn, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.getDataset import getIteration\n",
    "from data.vocab import computeInferenceData_Cognates\n",
    "\n",
    "sources = computeInferenceData_Cognates(getIteration(3)[:24], vocabulary)\n",
    "print(sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.vocab import oneHotsToWords\n",
    "\n",
    "testIndexInBatch = 20\n",
    "word = oneHotsToWords(sources[0][:,testIndexInBatch:testIndexInBatch+1], False, vocabulary)[0]\n",
    "print(f\"word for test: {word}\\n\")\n",
    "print(f\"word IntTensor: {sources[0][:, testIndexInBatch]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN LM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda import is_available\n",
    "from lm.PriorLM import CharLM\n",
    "\n",
    "device = 'cuda' if is_available() else 'cpu'\n",
    "\n",
    "# Init the character level LSTM language model \n",
    "LSTM_lm = CharLM(embedding_size=1024, hidden_size=100, num_layers=2, dropout_rate=0.1, vocab=vocabulary).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***`TODO`***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = LSTM_lm.inference(sources)\n",
    "print(f\"p('{word}') =\", probs[testIndexInBatch].item())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $n$-gram LM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lm.PriorLM import NGramLM\n",
    "\n",
    "bigram_20k = NGramLM(n=2, vocab=vocabulary)\n",
    "bigram_10k = NGramLM(n=2, vocab=vocabulary)\n",
    "bigram_5k = NGramLM(n=2, vocab=vocabulary)\n",
    "\n",
    "trigram_20k = NGramLM(n=3, vocab=vocabulary)\n",
    "trigram_10k = NGramLM(n=3, vocab=vocabulary)\n",
    "trigram_5k = NGramLM(n=3, vocab=vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_test = \"absyrd ä ifikare\"\n",
    "batch = bigram_20k.batch_ngram(sentence_test)\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Torch MP\n",
    "bigram_20k.train(tokens_20k)\n",
    "bigram_10k.train(tokens_10k)\n",
    "bigram_5k.train(tokens_5k)\n",
    "\n",
    "trigram_20k.train(tokens_20k)\n",
    "trigram_10k.train(tokens_10k)\n",
    "trigram_5k.train(tokens_5k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_20k.inference(sources)\n",
    "bigram_10k.inference(sources)\n",
    "bigram_5k.inference(sources)\n",
    "\n",
    "trigram_20k.inference(sources)\n",
    "trigram_10k.inference(sources)\n",
    "trigram_5k.inference(sources)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
