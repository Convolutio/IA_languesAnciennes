{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import getcwd, chdir\n",
    "\n",
    "if getcwd().endswith('notebooks'):\n",
    "    chdir('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get cognate Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the generation, the cognates already need to be transformed into the ByteTensor format. Each ByteTensor in the list of dictionnaries has the elementary shape $\\left( |y_{c, l}|\\right)$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import tensor, Tensor, uint8\n",
    "from uneurecon.data.vocab import get_vocabulary\n",
    "from uneurecon.data.getDataset import getCognatesSet\n",
    "from uneurecon.Source.utils import dl_to_ld\n",
    "from uneurecon.models.types import ModernLanguages\n",
    "\n",
    "vocabulary = get_vocabulary()[0]\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"On CUDA device:\", device == \"cuda\")\n",
    "\n",
    "cognates: list[dict[ModernLanguages, Tensor]] = [{lang:tensor(data=vocabulary(list(d[lang])), dtype=uint8, device=device) for lang in d} for d in dl_to_ld(getCognatesSet())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling hyparametres\n",
    "Please choose the hyperparametres for this sampling run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "parametres"
    ]
   },
   "outputs": [],
   "source": [
    "GENERATE_WITH_ALGORITHM = False\n",
    "SAMPLES_NUMBER_PER_COGNATE_GROUP = 2000\n",
    "MINI_BATCH_SHAPE = (len(cognates)//8, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Samples generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*One of the two proposed generation methods for the test is being executed according to the value of the `GENERATE_WITH_ALGORITHM` constant.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples: list[Tensor] = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Random prototype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uneurecon.Tests.createSamples import createSamplesBatch\n",
    "if not GENERATE_WITH_ALGORITHM:\n",
    "    samples = createSamplesBatch(len(cognates), SAMPLES_NUMBER_PER_COGNATE_GROUP, device, vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. With the generation algorithm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose the first Bouchard-Côté model's iteration for the reconstructions from which the generation will be processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uneurecon.data.getDataset import getIteration\n",
    "from uneurecon.Source.generateProposals import generateProposalsFromCurrentReconstructions\n",
    "\n",
    "if GENERATE_WITH_ALGORITHM:\n",
    "    currentReconstructions: list[Tensor] = [tensor(data=vocabulary(list(word)), dtype=uint8, device=device) for word in getIteration(1)]\n",
    "    samples = generateProposalsFromCurrentReconstructions(currentReconstructions, cognates, SAMPLES_NUMBER_PER_COGNATE_GROUP)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uneurecon.Source.reconstructionModel import ReconstructionModel\n",
    "from uneurecon.models.types import MODERN_LANGUAGES\n",
    "\n",
    "LSTM_INPUT_DIM = 50\n",
    "LSTM_HIDDEN_DIM = 50\n",
    "\n",
    "randomEditModel = ReconstructionModel(MODERN_LANGUAGES, vocabulary, LSTM_INPUT_DIM, LSTM_HIDDEN_DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "False Language Model with neutral probability for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uneurecon.models.types import InferenceData_Samples\n",
    "from uneurecon.lm.PriorLM import PriorLM\n",
    "from torch import zeros, float32\n",
    "\n",
    "class LM(PriorLM):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def inference(self, reconstructions: InferenceData_Samples):\n",
    "        return zeros(size=reconstructions[1].size(), dtype=float32, device=device)\n",
    "    \n",
    "random_lm = LM()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute unnormalized probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset of roughly $\\frac{B \\cdot C}{b \\cdot c}$ mini batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uneurecon.data.reconstruction_datasets import samplingDataLoader\n",
    "\n",
    "dataloader = samplingDataLoader(samples, cognates, vocabulary, MINI_BATCH_SHAPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(device==\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The unnormalized probabilities are computed from by running the inference in the prior language model and the forward dynamic program for each edit model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Example:</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.profiler import profile, ProfilerActivity, record_function\n",
    "\n",
    "acts = [ProfilerActivity.CPU]\n",
    "if device == \"cuda\":\n",
    "    acts.append(ProfilerActivity.CUDA)\n",
    "with profile(activities = acts, use_cuda = device==\"cuda\") as prof:\n",
    "    with record_function(\"data loading\"):\n",
    "        elt = next(iter(dataloader))\n",
    "    with record_function(\"reconstruction_model_inference\"):\n",
    "        edit_models_results = randomEditModel.forward_dynProg(*elt[0])\n",
    "    with record_function(\"prior_lm_inference\"):\n",
    "        prior_lm_results = random_lm.inference(elt[0][0])\n",
    "print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "t1 = time()\n",
    "elt = next(iter(dataloader))\n",
    "edit_models_results = randomEditModel.forward_dynProg(*elt[0])\n",
    "prior_lm_results = random_lm.inference(elt[0][0])\n",
    "t2 = time()\n",
    "dt = t2 - t1\n",
    "total_time = dt*(SAMPLES_NUMBER_PER_COGNATE_GROUP*len(cognates))/(len(cognates)//8 * 50)\n",
    "print(f\"Pour une itération : {dt}s ; Au total sur le dataset : {total_time//3600}h{total_time//60}m\")\n",
    "\n",
    "for lang in edit_models_results:\n",
    "    print(f'Probs tensor shape from {lang}\\'s edit model: {edit_models_results[lang].size()}')\n",
    "print(f'Probs tensor shape from prior edit model:', prior_lm_results.size())\n",
    "\n",
    "unnormalized_probs = prior_lm_results\n",
    "for lang in edit_models_results:\n",
    "    unnormalized_probs += edit_models_results[lang]\n",
    "print('Unnormalized probs tensor shape:', unnormalized_probs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Complete iteration:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for data in dataloader:\n",
    "    __edit_models_results = randomEditModel.forward_dynProg(*data[0])\n",
    "    __prior_lm_results = random_lm.inference(data[0][0])\n",
    "    __results = __prior_lm_results\n",
    "    for lang in __edit_models_results:\n",
    "        __results += __edit_models_results[lang]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (nlp_pytorch)",
   "language": "python",
   "name": "nlp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
