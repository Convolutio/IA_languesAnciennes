from torch import Tensor
from typing import Literal

from data.vocab import keepWordsInVoc, wordsToOneHots
from models.types import ModernLanguages


def getCognatesSet() -> dict[ModernLanguages, list[str]]:
    """
    Returns a dictionary containing the raw cognates in padded IntTensors, formatted as one-hot indices (without boundaries).
    """
    cognates: dict[ModernLanguages, list[str]] = {}
    for modernLanguage in ('french', 'spanish', 'portuguese', 'romanian', 'italian'):
        with open(f"./recons_data/data/{modernLanguage.capitalize()}_ipa.txt", "r", encoding="utf-8") as f:
            # Remove 1st and last characters, i.e., space and line break.
            # Do not take the last line, which is just a space.
            cognates[modernLanguage] = [line[1:-1] for line in f.readlines()[:-1]]

    return cognates


def getTargetsReconstruction() -> list[Tensor]:
    """
    Returns a list of target reconstructions in padded IntTensors, formatted as one-hot indices (without boundaries).
    """
    with open(f"./recons_data/data/latin_ipa.txt", "r", encoding="utf-8") as f:
        # Eliminate the ' ' and the '\n', and the last line.
        return [wordsToOneHots([line[1:-1]]).squeeze(1) for line in f.readline()[:-1]]


def getIteration(i: Literal[1,2,3,4]) -> list[str]:
    """
    Returns the samples from the i-th Bouchard-CÃ´te et al.'s model iteration in a padded IntTensor, formatted as one-hot indices (without boundaries).
    """
    iteration:list[str]
    with open(f'./recons_data/iteration3_{str(i)}.txt', 'r', encoding='utf-8') as file:
        iteration = [line[:-1] for line in file.readlines()[:-1]]

    return iteration


def getLMTrainingSet(setSizes: list[int]) -> list[str]:
    """
    Generate training sets for a language model, of a size given by `setSizes`, as a list of strings.

    Args:
        setSizes: list of sizes (int) for each training set.

    Returns:
        list(str): list of strings representing the training sets.
    """

    # Vocabulary generated by `extractIPA.py`
    voc: str
    with open('./data/IPA_vocabulary.txt', 'r', encoding='utf-8') as vocFile:
        voc = vocFile.read().replace(', ', "")

    # Dataset source : https://huggingface.co/datasets/pstroe/cc100-latin
    rawData: list[str]
    with open('./latin_text_ipa.txt', 'r', encoding='utf-8') as dataFile:
        rawData = dataFile.read().split(' ')

    trainingSet: list[str] = []
    for size in setSizes:
        trainingSet.append(keepWordsInVoc(rawData, voc, size,
                           filename=f"{size}_latin_tokens"))

    return trainingSet