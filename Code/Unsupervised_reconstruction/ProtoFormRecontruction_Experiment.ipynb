{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "9N66w8WAeYxN"
      },
      "source": [
        "# Neural unsupervised approach for proto-form reconstruction : experiment about the impact of the language model.\n",
        "\n",
        "Authors : Benjamin BADOUAILLE, Eliott CAMOU, Thomas HORRUT (Université de Bordeaux, CPBx)\n",
        "\n",
        "Supervised by : Rachel Bawden (Inria, Paris, ALMAnaCH group)\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "This notebook contains an implementation of the proto-form reconstruction neural model that [Andre He et al.](https://arxiv.org/abs/2211.08684) designed. This implementation covers the training, the inference and the evaluation of the general model, with also the training of several language models of the proto-language. Some details and tests about generation, sampling and maximisation steps could be addressed in other [notebooks](./notebooks/).\n",
        "\n",
        "The aim of the experiment is to reproduce the latin reconstruction as in the authors' paper, with similar experimental conditions. The main change is that the training and the evaluation will be carried out several times with differently-configured prior language models. Then, the models' performances could be compared in function of these language models.\n",
        "\n",
        "*This experiment is led in the context of an end-of-preparatory-cycle scientific research project, whose analysis will fed a wider reflexion about AI potential and limitations for solving Historical Linguistics problems.*"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-MvHc45wJY77"
      },
      "source": [
        "# Framework's datasets"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "XeoAJmIFJk8k"
      },
      "source": [
        "\n",
        "\n",
        "*   $L$ is the roman languages set with which we work (`french`, `romanian`, `spanish`, `italian`, `portuguese`).\n",
        "*   $\\Sigma$ is the IPA characters set which is used in our tokenisation. With the special characters `'('` and `')'`, it constitutes the input vocabulary for the language and edit models. The insertion edit model ($q_\\textrm{ins}$) returns a probability distribution over $\\Sigma \\cup \\{\\textrm{\"<del>\"}\\}$ and the substition one ($q_\\textrm{sub}$) returns over $\\Sigma \\cup \\{\\textrm{\"<del>\"}\\}$\n",
        "*   Let $C$ be the cognates pairs set. We note the batch size $B := |C|$. \n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ww6npXCfqHoZ"
      },
      "source": [
        "## $Σ$ initialisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sd5Bt-NOhBY7",
        "outputId": "037d6172-74cc-4474-9dc9-f4d001049347"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "IPA characters : \n",
            "/  / / z / / ɣ / / m / / ɒ / / u / / s / / ʲ / / ˈ / / ː / / ʔ / / ɨ / / o / / k / / p / / e / / a / / β / / ø / / b / \n",
            "/ f / / ɡ / / ʒ / / y / / ɲ / / ɾ / / ˌ / / ɛ / / d / / ɹ / / w / / x / / n / / l / / r / / œ / / ɐ / / ʁ / / v / / ʌ / \n",
            "/ ʊ / / ŋ / / ʝ / / ʰ / / ʎ / / j / / h / / ð / / ʃ / / ɪ / / - / / ɔ / / ̃ / / ə / / ɑ / / i / / θ / / t / / ɥ / "
          ]
        }
      ],
      "source": [
        "from data import vocab\n",
        "\n",
        "IPA_VOC = vocab.SIGMA_INV\n",
        "\n",
        "print(\"IPA characters : \")\n",
        "for i in range(len(IPA_VOC)):\n",
        "    print(f'/ {str(IPA_VOC[i])} /', end = ' ')\n",
        "    if i%20==19: print()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preprocessing of $C$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/autofs/unitytravail/travail/thorrut/IA_languesAnciennes/Code/Unsupervised_reconstruction\n"
          ]
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: './recons_data/data/french_ipa.txt'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[4], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mos\u001b[39;00m \u001b[39mimport\u001b[39;00m getcwd\n\u001b[1;32m      8\u001b[0m \u001b[39mprint\u001b[39m(getcwd())\n\u001b[0;32m----> 9\u001b[0m cognates: \u001b[39mdict\u001b[39m[ModernLanguages, InferenceData] \u001b[39m=\u001b[39m {language:computeInferenceData(pad_sequence([wordToOneHots(w) \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m wList], batch_first\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)) \u001b[39mfor\u001b[39;00m (language, wList) \u001b[39min\u001b[39;00m getCognatesSet()\u001b[39m.\u001b[39mitems()}\n",
            "File \u001b[0;32m/autofs/unitytravail/travail/thorrut/IA_languesAnciennes/Code/Unsupervised_reconstruction/data/getDataset.py:7\u001b[0m, in \u001b[0;36mgetCognatesSet\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m cognates:CognatesSet \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mfrench\u001b[39m\u001b[39m\"\u001b[39m:[], \u001b[39m\"\u001b[39m\u001b[39mspanish\u001b[39m\u001b[39m\"\u001b[39m:[], \u001b[39m\"\u001b[39m\u001b[39mportuguese\u001b[39m\u001b[39m\"\u001b[39m:[], \u001b[39m\"\u001b[39m\u001b[39mitalian\u001b[39m\u001b[39m\"\u001b[39m:[], \u001b[39m\"\u001b[39m\u001b[39mromanian\u001b[39m\u001b[39m\"\u001b[39m:[]}\n\u001b[1;32m      6\u001b[0m \u001b[39mfor\u001b[39;00m modernLanguage \u001b[39min\u001b[39;00m cognates:\n\u001b[0;32m----> 7\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m./recons_data/data/\u001b[39;49m\u001b[39m{\u001b[39;49;00mmodernLanguage\u001b[39m}\u001b[39;49;00m\u001b[39m_ipa.txt\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m\"\u001b[39;49m, encoding\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m) \u001b[39mas\u001b[39;00m file:\n\u001b[1;32m      8\u001b[0m         lines \u001b[39m=\u001b[39m file\u001b[39m.\u001b[39mreadlines()\n\u001b[1;32m      9\u001b[0m         \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(lines)\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m):\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './recons_data/data/french_ipa.txt'"
          ]
        }
      ],
      "source": [
        "from data.getDataset import getCognatesSet\n",
        "from data.vocab import wordToOneHots, computeInferenceData\n",
        "from Types.articleModels import ModernLanguages\n",
        "from Types.models import InferenceData\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "cognates: dict[ModernLanguages, InferenceData] = {language:computeInferenceData(pad_sequence([wordToOneHots(w) for w in wList], batch_first=True)) for (language, wList) in getCognatesSet().items()}"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.10 (PyTorch)",
      "language": "python",
      "name": "notebook_torch_env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
