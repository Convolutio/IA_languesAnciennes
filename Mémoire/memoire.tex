\documentclass[12pt, french, twoside]{report}
%\usepackage[total={6.5in,10in}, top=0.8in, left=1in, includefoot]{geometry}
\usepackage[a4paper, total={6.5in, 10in}, top=0.8in, left=1in,
    headheight=48pt,
    includefoot, includehead]{geometry}
\usepackage{hyperref} % les hyperliens !!!
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{mathptmx} %Times font (contraint par le CPBx)
\usepackage{amsmath} % for matrices print

\usepackage{babel, csquotes, xpatch}
%\usepackage{polyglossia}
\usepackage[backend=biber, style=numeric]{biblatex}
\addbibresource{biblio.bib}

\usepackage{amsfonts}
\usepackage{fancyhdr} % Pour la mise en page des en-têtes + pieds de page
\usepackage{lastpage} % pour avoir accès à la dernière page
\usepackage[toc]{glossaries} % glossaire auto, on peut s'y référer dans le latex avec \Gls{maRef}

% mettre les éléments du glossaire ici
\newglossaryentry{edit_dist}{
        name={distance},
        description={C'est le nombre minimal de modifications (insertion, substitution, suppression) à appliquer à une chaîne de caractères $a$ pour obtenir la chaîne de caractères $b$.}
}
\makeglossaries
\pagestyle{fancy}
\fancyhf{}
\fancyhead[RO]{\nouppercase{\hfill\leftmark}}
\fancyhead[LE]{\nouppercase{\rightmark\hfill}}
\fancyfoot[L]{Mémoire CPBx}
\fancyfoot[C]{\thepage/\pageref{LastPage}}
\fancyfoot[R]{2023}

\begin{document}
\begin{titlepage}
    \centering
    \vspace*{\fill}

    \huge\bfseries
    Les utilisations possibles de l'Intelligence Artificielle dans la linguistique historique
    
    \vspace*{1.5cm}
    \large 3 étudiants de CPBx
    
    \vspace*{\fill}
\end{titlepage}

\null
\setcounter{page}{1}
\thispagestyle{empty}
\newpage % Première page blanche pour la numérotation (contrainte de mise en page)
\section{Résumé}
\section{Abstract}
\section{Remerciements}


\tableofcontents
\listoffigures
\listoftables %si inutile, on le virera

\printglossary % TODO: faire marcher ça

\chapter{Introduction}
\textit{Mise en contexte pour arriver à la problématique, quel est le potentiel de l'intelligence artificielle dans la linguistique historique ? ... ?}\Gls{edit_dist}

\chapter{La linguistique historique et l'Intelligence Artificielle}
\section{La linguistique historique}
\subsection{Introduction à la linguistique historique}
\textit{Définir ce qu'est la linguistique historique, ce qu'elle étudie, et les mots de vocabulaires que nous allons rencontrer tout au long du mémoire.}
\subsection{Les différents principes}
\textit{Évidemment cette science repose sur des concepts, allant des propriétés synchoniques des mots aux à leurs aspects diachroniques.}
\subsection{Les atouts de l'Intelligence Artificielle dans ce domaine}
\textit{La linguistique historique fait face à de nombreux problèmes récurrents (traiter une grande quantité de textes pour l'homme, remarquer des motifs dans ces documents historiques). Alors que ce travail pourrait être effectué par une machine, grâce à sa capacité à traiter un grand nombres de données, et à chercher des similarités dans ces données. Avant, de voir les tâches où l'Intelligence Artificielle peut intervenir, il est d'abord nécessaire de voir en détail la conception des ces IA.}

Résoudre des problèmes de Linguistique Historique avec un ordinateur nécessite de lui faire traiter
du contenu textuel devant être abstrait sur des terrains parmi ceux de la \textbf{phonétique}, de la
\textbf{sémantique}, de la \textbf{morphologie} ou encore de la \textbf{syntaxe}.\\
\textbf{\textit{Développper un exemple pour illustrer ces 4 niveaux d'abstractions}}

La réalisation de ces abstractions s'inscrit dans le Traitement Automatisé du Langage Naturel (TAL),
un domaine à cheval entre la Linguistique et l'Informatique. L'Intelligence Artificielle y occupe
une place centrale pour sa capacité à effectuer des approximations améliorables avec de l'entraînement.


\section{L'IA dans le Traitement Automatisé du Langage Naturel}
\subsection{Introduction à l'apprentissage automatique}
\textit{Qu'est ce qu'une intelligence artificielle ?\\
    Qu'est ce qu'un réseau de neurones ?\\
    Quel est le principe derrière l'apprentissage automatique ?\\
    Définition des apprentissages supervisés/non supervisés
    Définition de propagation avant.
    Définition rétro-propagation du gradient.
    Exemple de FFNN pour tâche de classification}

Un important nombre de problèmes informatiques peut être résolu à travers la détermination d'une fonction mathématique $f$ d'un espace vectoriel $\mathbb{K}^n$ vers un espace vectoriel $\mathbb{K}^{n'}$ (avec $\mathbb{K}$ correspondant à $\mathbb{R}$ ou $\mathbb{C}$).\\
Lorsqu'un algorithme conventionnel est développé pour réaliser un tâche, $f$ a déjà implicitement été trouvé. Par exemple, derrière un traitement opéré sur une chaîne de caractères, elle existe bien, avec pour entrée une séquence de $n$ caractères encodés sous forme de bits qui forme un vecteur de l'espace $\mathbb{R}^n$ et pour sortie un élément d'un espace $\mathbb{R}^{n'}$ représentant la chaîne de sortie.\\
En revanche, de nombreux cas demeurent où il est difficile -- voire impossible -- de poser une expression mathématique ou un algorithme pour répondre à certains problèmes. On considère alors $f$ comme hypothétique et on cherche à l'approcher à partir d'un \textbf{modèle}, qu'on construit à partir des informations qu'on dispose sur $f$, comme un ensemble de ses points $\{(x_k, y_k=f(x_k)), k \in S\}$, à travers une tâche dite de \textbf{régression}.

\vspace{12pt}
Les \textbf{réseaux de neurones} sont des outils performants pour établir des modèles. Mathématiquement, ce sont des compositions d'applications non-linéaires et linéaires recevant un vecteur d'entrée représentant une donnée et sortant un vecteur de sortie représentant un résultat dans un format cohérent avec le problème.

\subsubsection{Définitions, du neurone au réseau}
Le neurone artificiel le plus élémentaire effectue la \textbf{somme pondérée} des coefficients du vecteur d'entrée, à laquelle il ajoute une valeur de \textbf{biais} pour enfin calculer l'image de la somme à travers une fonction non-linéaire dite \textbf{d'activation}. La sortie du neurone est donc un réel ou un complexe. Si on la note $y_i$, qu'on note $x$ le vecteur d'entrée dans $\mathbb{K}^n$, $w_i$ le vecteur de \textbf{poids} associé au neurone, $b_i$ son biais et $\sigma$ sa fonction d'activation, on a :
\begin{equation}
    y_i = \sigma(b_i + \sum_{j=0}^{n} w_{ij}x_j) = \sigma(b_i + <w_i, x>)
\end{equation}\cite[section 1]{jurafsky_ffnn}

Une \textbf{couche de neurones} est la mise en commun d'un nombre abritraire $N$ de neurones devant prédire des sorties $y_i$ différentes. Leurs vecteurs de poids $w_i$ diffèreront donc. En revanche, leur fonction d'activation est identique. La sortie d'une couche est donc un vecteur $y$ pouvant s'écrire comme dans l'équation \ref{def_couche}\footnote{On y confond la fonction d'activation avec la fonction vectorielle $\sigma$ s'appliquant indépendamment à chaque coefficient.}.

\begin{equation} \label{def_couche}
    \begin{split}
        y & =
        \left(\begin{matrix}
            \sigma(b_1 + <w_1, x>) \\
            \sigma(b_2 + <w_2, x>) \\
            \vdots \\
            \sigma(b_i + <w_i, x>) \\
            \vdots \\
            \sigma(b_N + <w_N, x>)
        \end{matrix}\right)
        =
        \sigma\left(\begin{matrix}
            b_1 + \sum_{j=0}^{n} w_{1j}x_j \\
            b_2 + \sum_{j=0}^{n} w_{2j}x_j \\
            \vdots \\
            b_i + \sum_{j=0}^{n} w_{ij}x_j \\
            \vdots \\
            b_N + \sum_{j=0}^{n} w_{Nj}x_j
        \end{matrix}\right)\\
        & = \sigma\left(
            \left(\begin{matrix}
                b_1 \\ b_2 \\ \vdots \\ b_i \\ \vdots \\ b_N
            \end{matrix}\right)
            +
            \left(\begin{matrix}
                w_{11} & w_{12} & \hdots & w_{1j} & \hdots & w_{1n} \\
                w_{21} & w_{22} & \hdots & w_{2j} & \hdots & w_{2n} \\
                \vdots & \vdots & \ddots & \vdots & \ddots & \vdots \\
                w_{i1} & w_{i2} & \hdots & w_{ij} & \hdots & w_{in} \\
                \vdots & \vdots & \ddots & \vdots & \ddots & \vdots \\
                w_{N1} & w_{N2} & \hdots & w_{Nj} & \hdots & w_{Nn} \\
            \end{matrix}\right)
            \left(\begin{matrix}
                x_1 \\ x_2 \\ \vdots \\ x_j \\ \vdots \\ x_n
            \end{matrix}\right)
        \right)
        = \sigma(b + Wx)
    \end{split}
\end{equation}

Un réseau neuronal est ainsi formé à partir de la mobilisation d'une ou plusieurs couches. L'intuition derrière l'utilisation de couches intermédiaires, qu'on nomme des \textbf{couches cachées}, est que la machine puisse être capable d'apprendre à construire des \textbf{représentations adéquates} des données pour effecuter la prédiction finalement voulue avec pertinence. On parle alors d'\textbf{apprentissage profond} et cette technique offre des réponses face aux difficultés d'abstraction soulevées par les problèmes de TAL.

L'agencement des couches et la manière de calculer la sortie finale à partir de chacune d'elles, qu'on peut rassembler sous le terme de "mode de \textbf{propagation}", est un \textbf{paramètre architecturale} à part entière qu'il faut judicieusement définir en fonction de la tâche à réaliser. Pour traiter de l'utilisation de l'IA dans le TAL, au moins deux principaux types de réseaux de neurones seront introduits au cours de ce chapitre, différant par la nature cyclique ou non de l'enchaînement de leurs couches internes.\cite[introduction + section 3]{jurafsky_ffnn} [\textit{ajouter illustration}]

\subsubsection{Processus d'apprentissage}
L'entraînement d'un réseau de neurones s'effectue à travers des \textbf{ajustements des poids et des biais dans chaque couche}, dans le cadre d'une \textbf{minimisation de fonctions de perte}, déterminée par les sorties temporairement prédites par le réseau. Ceci s'effectue par un algorithme de \textbf{descente du gradient}. Le calcul du gradient de la fonction de perte selon tous les poids du réseau s'effectue avec un algorithme de \textbf{rétropropagation de l'erreur}, une adaptation pour les réseaux de neurones de celui de la discrimination rétropropagative sur des graphes d'exécution.\cite[section 6]{jurafsky_ffnn}.

\textit{
    expliquer la diff entre le supervisé et le non-supervisé\\
    expliquer les choix à faire sur l'entraînement (optimiseurs, taille de batch et parallélisation, nombre d'époques)\\
}\cite[section 2.1.2]{fourrier}

\subsubsection{Concevoir correctement}

\textit{dresser une liste récapitulant les configurations à effectuer sur un réseau (topologie + paramètres d'entraînement) et introduire le paramètre d'initialisation}

\textit{expliquer que le choix des paramètres se fait expérimentalement, à partir d'intuitions basées sur des travaux antérieurs sur de la conception de réseaux}

\textit{expliquer comment évaluer un réseau}

\subsubsection{Exemple de la tâche de classification}
\textit{expliquer}
\subsection{Traitement des donnnées}

Avant de donner à un réseau de neurones, quelconque données récupérées pour son entraînement, il est nécessaire de préparer ces données. Le but étant de rendre ces données d'entrées correctes et compréhensibles pour notre intelligence artificielle. Cet préparation s'effectue suivant le type de tâche souhaité. Néanmoins, dans sa généralité, les étapes de préparation de données en TAL restent les mêmes.\\

% \footnote{À noter que nous parlons ici de \og grandes \fg quantités de données à analyser, mais ce mot est relatif à un être humain, et encore, il arrive que ce nombre soit restreint, seulement, pour une IA c'est le contraire, nous parlerons plutôt de d'IA à \og basses ressources \fg car ce nombre de données est très petits face à ce qu'elle peut gérer dans le monde moderne et la quantités de textes disponibles sur la toile.}
Tout d'abord, il faut normaliser (nettoyer) notre base de données qui peut être, par exemple, un corpus de textes, une liste de mots, provenant de la toile, ou d'un système de transcription audio-visuelle \footnote{Dans le cadre de ce mémoire, le but n'est pas de savoir comment parvient-on à extraire du texte à partir de ces sources, mais plutôt de savoir comment rendre ce texte compréhensible à une l'intelligence artificielle choisie.}
Dans ces données se trouvent évidemment des mots, avec des \og caractéristiques \fg de la  langue \footnote{Pour simplifier, nous avons pris l'exemple d'une base de données  monolingue, mais il est possible qu'elle comporte plusieurs langues. Dans tout les cas, le procédé sera le même, si ce n'est qu'il faut s'adapter à chaque langue.} tel que les ponctuations, les lettres en capitales, les chiffres, ou bien les caractères spéciaux (comme le dièse que l'on retrouve souvent dans les \textit{tweets}). Seulement, il arrive parfois que certains de ces éléments rajoutent inutilement de la complexité pour une machine, sans que cela apporte plus de sens, ou bien sont non-désirées (voir incorrectes), ainsi on décide de les supprimer (ou les remplacer) pour ne garder que ce qui nous intéresse. [Exemple]\\

%[lib nltk]
Après avoir nettoyé notre base de données, il faut segmenter notre texte en mots ou sous-mots, autrement dit, il faut tokeniser notre texte. La tokenisation correspond à la segmentation de chaînes de caractères (comme notre texte) en tokens (mots, sous-mots, ponctuations). Par la suite, on peut extraire l'ensemble des tokens uniques d'un texte que l'on nommera : vocabulaire.
% Nous pourrions discuter longuement de ce qui devrait être pris comme tokens ou non. Par exemple, si nous devons prendre en compte la ponctuation en tant que token. Mais aussi les clitiques si elles doivent être pris en compte, ou si elles doivent être développées. Pour cela, nous vous renvoyons au Chapitre 2 de \og Speech and Language Processing \fg de Jurasky.
% Il est intéressant d’évoquer que suivant les différents niveaux de tokenisations (partant du caractères, au sous-mots, jusqu’au mots entier) aide à mieux respectivement à comprendre la morphologie, la sémantique (suivant les sous-mots des algorithmes statistiques), et la syntaxe (suivant les sous-mots basé sur les règles). Et il a été montré qu’il était suffisant de segmenter des phrases en sous-mots pour analyser le langage de manière pertinente. (Thèse Fourrier)
Prenons un exemple \og J'aime les bananes\fg, une approche naïve est de tokeniser notre texte suivant les espaces \footnote{Vous remarquerez déjà que ce processus ne s'applique pas au langue comme le Japonais, ou le Chinois.} ce qui nous donne les tokens [\og J'aime \fg, \og les \fg, \og bananes. \fg] avec un vocabulaire de taille 3 (dans cette phrase tout les tokens sont uniques). Cependant, cette approche pose des problèmes, en commençant par le token \og bananes. \fg qui a la même signification avec ou sans point, mais qui sera considéré comme différent pour une IA. Nous pourrions ajouter la séparation suivant la ponctuation, mais alors nous obtiendrons pour \og J'aime \fg les tokens [\og J \fg, \og ' \fg, \og aime \fg] qui est une forme tout aussi problématique. Et il existe encore de nombreux cas (les abréviations, les points de suspension, etc.) où ce type de tokenisation pose problème. Une approche alternative est de tokeniser suivant des règles définis, par exemple, de prendre en compte les contractions comme \og J'aime \fg et de le transformer en deux tokens [\og Je \fg, \og aime \fg], qui est une méthode beaucoup plus efficace que la première approche, mais montrera des limites face à des situations (ou mots) rares, ou alors il faudrait spécifier de nouvelles règles pour gérer ces cas. Ainsi, la solution proposée est une approche statistique, consistant à décomposer de plus en plus un mot en sous-mots \footnote{Remarquez que le terme \textit{mot} a un sens différent que celui qui le précède.} au fil que sa fréquence diminue. [Exemple]. Les quatres algorithmes de tokenisation en sous-mots les plus utilisées sont le \textit{Byte-Pair Encoding} (Sennrich et al., 2016), l'\textit{unigram language modeling} (Kudo, 2018), le \textit{WordPiece} (Schuster et Nakajima, 2012), et le \textit{SentencePiece}\footnote{Cet algorithme est une implémentation des deux premiers.} (Kudo et Richardson, 2018).\\

Pour la tâche de classification, vous avez vu que les mots d'entrées, [exemple], étaient convertis en une liste de nombre, autrement dit un vecteur, sur lequel il a été effectué des calculs afin d'obtenir un résultat (un nouveau vecteur), [exemple]. Une machine, un réseau de neurones, ne comprend que des nombres et ne sait procéder qu'à des calculs. Il existe différentes façons de convertir un token en un vecteur numérique, mais on retiendra deux méthodes l'encodage 1 parmi n et le plongement lexical (respectivement et plus communément appelés en anglais le \textit{one-hot encoding} et le \textit{word embedding}).\\
% Discuter qu'on transpose le sens du mot en vecteur.
% Ressemble bcp à Jurasky chap 7
\indent L'encodage 1 parmi n consiste à créer un vecteur binaire de la taille du vocabulaire $|V|$, c'est-à-dire, que chaque dimension correspond à un mot dans le vocabulaire. Dans ce vecteur binaire, la valeur est 1 pour la dimension correspondant au mot dans le vocabulaire, et 0 pour toutes les autres dimensions. Ainsi, en supposant que la dimension du mot \og bananes\fg se trouve à la troisième dimension (autrement dit c'est le troisième mot de notre vocabulaire $V$), sa représentation correspondra à un 1 à la troisième dimension et à des 0 sur les autres, soit le vecteur : %Jurasky Chap 7
\begin{align*}
    \begin{bmatrix}
    0 & 0 & 1 & 0 & \dots & 0
    \end{bmatrix}\\
    \begin{matrix}
    1 & 2 & 3 & 4 & \dots & |V|
    \end{matrix}
\end{align*}

Comme vous pouvez le voir l'avantage de cet encodage est sa simplicité de mise en oeuvre. Mais le problème survient quand le vocabulaire devient très grand, les vecteurs générés, par définition, deviennent à leurs tours très grands et très dispersés (beaucoup de 0 et peu de 1), ce qui entraine une augmentation de la complexité du modèle (le nombre de dimension pour décrire les données) et des temps de calcul. De plus, cet encodage ne prend pas en compte les relations sémantiques entre les mots, on ne peut mesurer la similarité entre les mots,  car ils sont encodés de façon indépendante les uns des autres, ce qui peut être particulièrement problématique pour les nombreuses tâches de TAL tel que la compréhension ou traduction d'une langue.

Le plongement lexical, en revanche, permet de représenter les mots en encapsulant leur \textit{sens sémantique} par des vecteurs denses de plus faibles dimensions, c'est à dire des vecteurs de nombres réels de petites tailles. Pour obtenir la représentation des tokens en vecteur sémantique, on utilise une matrice d'enchassement.\footnote{En réalité, les tokens sont d'abord transformer en vecteur binaire ou en un nombre correspondant à l'index dans le vocabulaire, puis on applique la matrice enchassement.}. Cette matrice contient, à chaque ligne, les vecteurs de plongement lexical pour chaque mot du vocabulaire, et à chaque colonne correspond une dimension du vecteur de plongement. Par ailleurs, les dimensions de ces vecteurs n'ont pas une représentation claire (Jurasky, Chap 6). Cette matrice représente un espace vectoriel pour les mots du vocabulaire, c'est à dire que les propriétés des vecteurs vont pouvoir s'appliquer pour nos mots vectorisés. Par exemple, étudier la similarité entre deux mots revient à calculer la distance entre les deux vecteurs correspondants, ou encore comme la figure X le montre, les vecteurs peuvent s'additionner/se soustraire permettant d'obtenir un nouveau vecteur qui aura garder une cohérence sémantique face à ces opérations. [Figure : Exemple de vecteurs sémantiques montrant bien qu'ils portent un sens et permettent des relations sémantiques entre eux ; vecteur "king" $-$ "man" $+$ "women" $->$ "queen"] Pour obtenir cette matrice d'enchassement, deux méthodes sont possibles, soit elle est crée par entrainement par notre propre modèle, soit elle est crée par un modèle externe (tel que Word2Vec, GloVe, BERT, etc.).

% Data splitting and batching

\subsection{Architectures neuronales utiles au TAL}
\textit{Quels sont les différents outils ?}
\textit{Suivant, comment les parties précédentes ont été traités, ou comment les parties futures seront discutées, cette partie pourrait ne pas être nécessaire. Sinon, elle regroupera l'idée de comment on passe de notre langue naturelle à celle de la machine, de passer aux mots à des vecteurs ? Quels traitements théoriques (théoriques pour ce distinguer de la pratique dans la partie future) doient être effectués sur les mots ? En fait cette partie fait référence aux chapitres 2 et 6 de Jurasky. Voir même le chapitre 9, en supprimant la sous partie précendente pour pouvoir parler directement des réseaux de neurones appliqués à la linguistique, en d'autres termes, des réseaux récurrents, des modèles séquentielles (encodeurs-décodeurs) avec l'attention, et des Transformers.}\\

\subsubsection{Réseaux de neurones récurrents}

...

% Transition vers les Transformeurs, discuter des problèmes que pose les RNN. 

\subsubsection{Transformeurs}

Précédemment, avec les RNN et les LTSM, nous avons introduit le mécanisme d'attention, permettant au réseau de se focaliser sur la manière dont les mots (éloignés) sont reliées les uns aux autres. Seulement, comme nous l'avons vu aussi, ces réseaux se basent sur des connexions récurrentes, rendant le calcul coûteux et la parallélisation difficile. Ainsi, pour pallier ce problème et gagner en performance, un nouveau modèle de réseau de neurones apparait sous le nom de \textit{Transformers} (Transformeurs en français) dans le papier \og Attention Is All You Need \fg\; de Vaswani et al. (2017). Ce modèle de type encodeur-décodeur se base sur l'attention multi-têtes, l'innovation majeure des \textit{Transformers}\footnote{Par simplification, nos efforts se concentrerons sur l'attention multi-têtes, sans évoquer qu'un \textit{Transformers} se décompose en blocs de \textit{transformer}, dont chaque bloc contient une unité d'attention multi-têtes (masqué ou non) et un FFNN, accompagné de connexions résiduelles et des couches de normalisation. Pour plus de détails, nous vous invitons à regarder le papier de Vaswani et al. (2017).}.\\

L'attention multi-têtes permet d'étudier tous les vecteurs d'entrées, comme des mots, en même temps (de façon parallèle), et dont chaque tête $h$ qui la compose se focalise sur un aspect des \textit{interactions} entre les différents éléments de la séquence, [exemple].\\ 

Chaque tête contient un module d'auto attention (\textit{self-attention} en anglais) qui est utilisé pour permettre à chaque tokens $x_i$ de pouvoir \textit{intéragir} avec tous les autres tokens de la séquence $X$. Pour cela on extrait pour chaque $x_i$ un trio de vecteurs : un vecteur requête $q_i$, un vecteur clé $k_i$, un vecteur valeur $v_i$. Le vecteur requête (\textit{query}) correspondant au vecteur sur lequel on porte notre attention et qui sera comparé à tous les autres vecteurs, nommés les vecteurs de clés (\textit{keys}). Puis, nous avons le vecteur valeur(\textit{value}) qui sera utilisé pour représenter la \og valeur sémantique (du mot)\fg\; et sera multiplié par le poids d'attention calculé avec les autres vecteurs (requêtes et clés). [Figure : Exemple de la formation des différents vecteurs $q_i$, $k_i$, $v_i$ suivant une phrase X $[x_1, x_2, \dots, x_i, \dots, x_n]$]. Pour créer ces vecteurs, les vecteurs $x_i$ sont multipliés avec les matrices d'enchassement ($W^Q$, $W^K$, $W^V$) qui sont obtenus à l'entrainement du modèle : 
\[ q_i = W^Q x_i \,;\; k_i = W^K x_i \,;\; v_i = W^V x_i\]
On pose les matrices $Q$, $K$, $V$ respectivement l'ensemble des vecteurs $q_i$, $k_i$, $v_i$.\\

Ensuite, le calcule d'auto-attention s'effectue par la multiplication du score d'attention $((QK^T)/\sqrt{d_k})$ normalisé par la fonction \textit{softmax} avec la matrice des valeurs $V$ :
\[Z_h\; =\; SelfAttention(Q, K, V)\; =\; softmax(\frac{QK^T}{\sqrt{d_k}})V\]
% Inspiré des notations de l’article de Jay Alammar : The illustrated transformer.

Enfin, toutes les matrices $Z_h$, de chaque tête $h$, sont concaténées en une seule grande matrice qui est multipliée par une matrice $W^O$ (également obtenus à l'entrainement) pour former la matrice résultante de l'attention multi-têtes $Z$. [Figure : résumant toute les étapes.]
\[MultiHeadAttention(X)\;=\;(Z_1 + Z_2 + \dots + Z_h)W^O \quad \text{h correspondant au nombre de têtes}\]

[Prenons un exemple : ...]

Cependant, au tout début, nous avons évoqué que le modèle étudié tout les tokens $x_i$ en même temps, entrainant que le modèle ne tient pas compte de l'ordre des tokens, une information pourtant capitale. Par conséquent, pour injecter l'information de la position de nos tokens dans les vecteurs d'entrées, le papier Vaswani et al. (2017) propose une solution en additionnant, à nos tokens vectorisés sémantiquement, des vecteurs positions créent à base de fonction sinus et cosinus \footnote{Nous invitons le lecteur à regarder le papier de Vaswani et al. (2017) pour se convaincre de la pertinence de ce choix d'encodage de position. Simplement, retenez que cela permet au modèle de s'entrainer avec la position relative des tokens, plutôt que la position absolue.}. Ainsi, cette méthode permet à notre modèle de travailler avec tous les tokens $x_i$ de notre phrase $X$ en même temps tout en ayant l'information de la position de chaque mot dans leur vecteur.

Dans certaines situations, notamment dans la tâche de modélisation de langue en TAL, où le but est de prédire le prochain mot d'une phrase, l'entrainement avec ce type de Transformeurs est assez innaproprié, car vous connaissez déjà le prochain mot de votre phrase vu que vous étudiez tous les mots en même temps. Pour résoudre ce problème, on applique un masque tel que les valeurs de la partie triangulaire supérieure de la matrice $QK^T$ sont remplacés par des $-\infty$ (qui seront transformés en 0 par la fonction \textit{softmax}). Cette variante se nomme l'auto-attention masquée. De ce fait, le champ de visibilité de notre modèle sera réduit au mot qu'il est entrain de voir et à ceux qu'il a déjà vu, et pourra prédire de façon plus correcte (sans triche) les prochains mots de nos phrases. [Figure : montrant la matrice réduite]

\chapter{Les contributions de l'IA dans la linguistique historique}
\textit{Petite introduction avant de passer au papiers.}

\section{Restoration de documents anciens}
% Ithaca
...

\section{Déchiffrement de langues anciennes}
% MIT
Une autre tâche possible par l'intelligence artificielle est le déchiffrement de langue anciennes...

\chapter{Étude du cas de l'application de l'IA pour la reconstruction des proto-formes d'une langue}
\textit{Ici, on se place dans un cas concret, pour montrer que ce n'est pas que de la théorie. En proposant une expérience.}
\section{État de l'art}
\subsection{Conceptualisation du problème}
\textit{Définir clairement le problème du titre, énoncer et justifier le choix de notre modèle réseaux de neurones et des différents outils appliqués. Voir s'il est possible de faire apparaitre plusieurs démarches, c'est à dire, une approche statistique et une approche neuronale (toujours pour renforcer et montrer le potentiel de l'IA).}

\subsection{Dernières solutions neuronales}
\textit{Solution supervisée + non supervisée}

\subsection{Limites d'applicabilité}
\textit{Expliquer en quoi le non-supervisé donne plus d'espoir que le supervisé mais en quoi même cette approche présente des limites.}\\
\textit{Transition avec la problématique de l'article scientifique}\\

\section{Observation expériementale d'une limite d'applicabilité d'une approche}
\subsection{Méthode}
\subsection{Récupération de la base de données}
\textit{Il est fort possible que cette partie se regroupe avec la partie suivante, car il n'y aura pas grand chose à dire.}

\subsection{Normaliser les données}

\subsection{Analyse}
\subsection{Critiques}
\textit{Il reste ici quelques sous parties à détailler.}

\chapter{Conclusion}
\section{Synthèse}
\textit{Résume tout ce qui a été dit.}
\section{Les différentes limites posées aujourd'hui}
\textit{une partie des limites aura déjà été traitée dans le chapitre précédent. Cette sous partie se veut résumer ces limites, et aller dans les limites générales (voir acutelles) de l'IA dans  la linguisitique historique.}
\section{Les perspectives de l'IA dans la linguistique historique}
\textit{Ouverture, dépassement de certaines limites, évolution des modèles.}

\chapter{Références}
\section{Bibliographie}

\printbibliography

\end{document}