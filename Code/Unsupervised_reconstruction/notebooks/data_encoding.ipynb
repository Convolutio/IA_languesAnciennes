{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import getcwd, chdir\n",
    "\n",
    "if getcwd().endswith('notebooks'):\n",
    "    chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.getDataset import getCognatesSet, getIteration\n",
    "from data.vocab import computeInferenceData_Samples, computeInferenceData_Cognates, wordsToOneHots\n",
    "from random import randint\n",
    "\n",
    "raw_cognates = getCognatesSet()\n",
    "__random_start_index = randint(0, len(raw_cognates['french'])-6)\n",
    "cognates = computeInferenceData_Cognates({lang: wordsToOneHots(raw_cognates[lang]) for lang in raw_cognates})['french']\n",
    "raw_cognates = raw_cognates['french'][__random_start_index: __random_start_index + 5]\n",
    "raw_samples = getIteration(1)[__random_start_index: __random_start_index + 5]\n",
    "samples = computeInferenceData_Samples(wordsToOneHots(raw_samples)) #TODO: simplify the data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Samples encoding: `InferenceData_Source` type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This type refers to a tuple of three elements:\n",
    "- an IntTensor `S` of shape $\\left(\\max \\{|x|, x \\in \\textrm{batch}\\} + 2, c, b\\right)$. For all $0 \\leq i < c$ and $0 \\leq j < b$, `S[:, i, j]` represents one sample among the $b$ ones which are linked with the $i$-th cognate pair. It is represented along the first axis by tokens encoded with one-hot indexes and the sequence is opened by the `SOS_TOKEN` and the `EOS_TOKEN`.\n",
    "- a cpu ByteTensor `L` of shape $\\left( c, b \\right)$ containing the length of each samples with the boundaries token. It is defined such that `S[L[i, j]:, i, j]` is a list of the `PADDING_TOKEN`'s one-hot indices. Therefore, `L[i, j]` = $|x_{(i,j)}| + 2$, if we note $x_{(i,j)}$ as the raw sample (without the boundaries token) represented at the position (i, j) in `S`.\n",
    "- `n`: the max of `L` (if the tuple is correctly defined, then `n = S.size()[0]`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ɔpɨradʊ', 'ɔpɨɾaθjɔ', 'ɔpɨɾasjɔ', 'ɔpɨɾator', 'ɔpɛrkylʊ']\n",
      "tensor([[58, 30, 13, 36, 14,  0,  2, 43, 57, 59],\n",
      "        [58, 30, 13, 36, 40,  0, 56,  7, 30, 57],\n",
      "        [58, 30, 13, 36, 40,  0, 15,  7, 30, 57],\n",
      "        [58, 30, 13, 36, 40,  0, 16, 12, 14, 57],\n",
      "        [58, 30, 13, 32, 14,  8, 21,  9, 43, 57]], dtype=torch.int32)\n",
      "torch.Size([10, 5, 1])\n",
      "\n",
      "Samples' length (without boundaries): [7, 8, 8, 8, 8]\n",
      "Samples' length (with boundaries): tensor([ 9, 10, 10, 10, 10])\n",
      "Max sample length with boundaries: 10\n"
     ]
    }
   ],
   "source": [
    "print(raw_samples)\n",
    "print(samples[0][...,0].T)\n",
    "print(samples[0].size())\n",
    "print('\\n' + \"Samples' length (without boundaries):\", str([len(c) for c in raw_samples]))\n",
    "print(\"Samples' length (with boundaries):\", samples[1][:,0])\n",
    "print(\"Max sample length with boundaries:\", samples[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cognates encoding: `InferenceData_Targets` type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This type of data is defined by a tuple similar with `InferenceData_Source`, excepted that the `EOS_TOKEN` is here removed from the first IntTensor, which involves that the sequences lengths are reduced by one, compared to the sequences in the previous type. Therefore, we can sum up its three elements in the following list:\n",
    "- `S`: an IntTensor of shape $\\left( \\max \\{ |y_l|, y_l\\in \\textrm{batch}_l \\} + 1, c \\right)$\n",
    "- `L`: a cpu ByteTensor of shape $(c)$ `L[i]` = $|y_{l, i}| + 1$\n",
    "- `n`: the max of `L` (if the tuple is correctly defined, then `n = S.size()[0]` )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['opeʁˈe', 'opeʁasjˈɔ̃', 'opeʁasjˈɔ̃', 'opeʁatˈœʁ', 'opɛʁkˈyl']\n",
      "tensor([[58,  3,  1,  ..., 59, 59, 59],\n",
      "        [58,  0,  1,  ..., 59, 59, 59],\n",
      "        [58,  0,  1,  ..., 59, 59, 59],\n",
      "        ...,\n",
      "        [58, 22, 51,  ..., 59, 59, 59],\n",
      "        [58,  2, 51,  ..., 59, 59, 59],\n",
      "        [58,  2, 51,  ..., 59, 59, 59]], dtype=torch.int32)\n",
      "torch.Size([19, 3213])\n",
      "\n",
      "Cognates' length (without SOS token): [6, 10, 10, 9, 8]\n",
      "Cognates' length (with SOS token): tensor([6, 6, 8,  ..., 5, 4, 4])\n",
      "Max cognate length with SOS token: 19\n"
     ]
    }
   ],
   "source": [
    "print(raw_cognates)\n",
    "print(cognates[0].T)\n",
    "print(cognates[0].size())\n",
    "print('\\n' + \"Cognates' length (without SOS token):\", str([len(c) for c in raw_cognates]))\n",
    "print(\"Cognates' length (with SOS token):\", cognates[1])\n",
    "print(\"Max cognate length with SOS token:\", cognates[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
