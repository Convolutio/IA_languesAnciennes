{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximisation-expectation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import getcwd, chdir\n",
    "\n",
    "# set the current working directory to the repository's root, if required\n",
    "if getcwd().endswith('notebooks'):\n",
    "    chdir('./../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tabulate in c:\\users\\thomas\\.pyenv\\pyenv-win\\versions\\3.9.10\\lib\\site-packages (0.9.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install tabulate\n",
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "pd.set_option('display.float_format', lambda x: f'{x:.4f}')\n",
    "\n",
    "def display_horizontaly_two_tables(df:pd.DataFrame,df2:pd.DataFrame, title1:str, title2:str):\n",
    "    df_md = df.to_markdown().split('\\n')\n",
    "    df2_md = df2.to_markdown().split('\\n')\n",
    "    title_line = title1 + \" \" * (len(df_md[0])-len(title1)) + \"\\t\" + title2\n",
    "    two_tabs = \"\\n\".join([title_line] + [df_md[i] + '\\t' + df2_md[i] for i in range(len(df_md))])\n",
    "    print(two_tabs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|y| max = 17\n",
      "|x| max = 16\n"
     ]
    }
   ],
   "source": [
    "from data.getDataset import getCognatesSet, getIteration\n",
    "from data.datapipes import formatTargets\n",
    "from data.vocab import computeInferenceData, wordsToOneHots, vocabulary, IPA_charsNumber\n",
    "from models.articleModels import ModernLanguages, MODERN_LANGUAGES\n",
    "from models.models import InferenceData\n",
    "from source.reconstructionModel import ReconstructionModel\n",
    "\n",
    "raw_cognates = getCognatesSet()\n",
    "cognates:dict[ModernLanguages, InferenceData] = formatTargets(raw_cognates)\n",
    "raw_samples = getIteration(1)\n",
    "currentReconstructions = computeInferenceData(wordsToOneHots(raw_samples).unsqueeze(-1)) #TODO: simplify the data loading\n",
    "\n",
    "LSTM_INPUT_DIM = 50\n",
    "LSTM_HIDDEN_DIM = 50\n",
    "\n",
    "randomEditModel = ReconstructionModel(MODERN_LANGUAGES, vocabulary, LSTM_INPUT_DIM, LSTM_HIDDEN_DIM)\n",
    "\n",
    "TEST_LANGUAGE:ModernLanguages = \"french\"\n",
    "x_maxLength = currentReconstructions[0].size()[0] - 2\n",
    "y_maxLength = cognates[TEST_LANGUAGE][0].size()[0] - 2\n",
    "print('|y| max =', y_maxLength)\n",
    "print('|x| max =', x_maxLength)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward dynamic Program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backward dynamic Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_prob = randomEditModel.backward_dynProg(currentReconstructions, cognates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test on $x :=$`\"absyrdʊ\"` and $y :=$`\"absˈyʁd\"` ($|x|=7$ and $|y|=7$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDX = 20 # the index in the batch of the sample to be studied\n",
    "x_length, y_length = 7, 7\n",
    "\n",
    "backward_prob_dlt = targets_prob[TEST_LANGUAGE].dlt[:,:,IDX].cpu().numpy().squeeze(-1)\n",
    "backward_prob_end = targets_prob[TEST_LANGUAGE].end[:,:,IDX].cpu().numpy().squeeze(-1)\n",
    "backward_prob_sub = targets_prob[TEST_LANGUAGE].sub[:,:,IDX].cpu().numpy().squeeze(-1)\n",
    "backward_prob_ins = targets_prob[TEST_LANGUAGE].ins[:,:,IDX].cpu().numpy().squeeze(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display of cached probabilities for the substitution and the deletion operations\n",
    "\n",
    "Notice that the logarithmic probabilities that have not to be defined for these operations are automatically set to $-\\infty{}$ thanks to the recurrence mathematical relations of the backward dynamic program. So do the undefined probabilities in padding positions, thanks to the neutrality of edit model's cached inference probabilities (which equal $0$ in the log space)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subsitution Operation                                                                                \tDeletion Operation\n",
      "|    |        0 |        1 |        2 |        3 |        4 |        5 |        6 |      7 |      8 |\t|    |        0 |        1 |        2 |        3 |        4 |        5 |        6 |         7 |      8 |\n",
      "|---:|---------:|---------:|---------:|---------:|---------:|---------:|---------:|-------:|-------:|\t|---:|---------:|---------:|---------:|---------:|---------:|---------:|---------:|----------:|-------:|\n",
      "|  0 | -1e+09   | -1e+09   | -1e+09   | -1e+09   | -1e+09   | -1e+09   | -1e+09   | -2e+09 | -2e+09 |\t|  0 | -1e+09   | -1e+09   | -1e+09   | -1e+09   | -1e+09   | -1e+09   | -1e+09   | -1e+09    | -2e+09 |\n",
      "|  1 | -1.35371 | -1.86939 | -2.35208 | -3.49419 | -4.72537 | -5.81715 | -7.59627 | -1e+09 | -1e+09 |\t|  1 | -1.80048 | -2.08853 | -2.44592 | -3.08607 | -4.10483 | -4.71093 | -6.2416  | -7.18322  | -1e+09 |\n",
      "|  2 | -2.28378 | -1.84261 | -1.96672 | -2.45948 | -3.33074 | -4.20092 | -5.86989 | -1e+09 | -1e+09 |\t|  2 | -2.75952 | -2.36094 | -2.22304 | -2.35782 | -2.98272 | -3.36245 | -4.58199 | -5.37401  | -1e+09 |\n",
      "|  3 | -3.27249 | -2.31927 | -2.04653 | -2.07499 | -2.58411 | -3.02769 | -4.54346 | -1e+09 | -1e+09 |\t|  3 | -3.67261 | -3.13285 | -2.40697 | -2.24389 | -2.50374 | -2.57778 | -3.4031  | -3.99823  | -1e+09 |\n",
      "|  4 | -4.03044 | -3.1999  | -2.28013 | -2.08759 | -2.27015 | -2.42872 | -3.38564 | -1e+09 | -1e+09 |\t|  4 | -4.87387 | -3.99302 | -3.06672 | -2.47484 | -2.43152 | -2.17711 | -2.58217 | -2.87902  | -1e+09 |\n",
      "|  5 | -5.19094 | -3.94673 | -3.05414 | -2.32763 | -2.25144 | -2.00502 | -2.58155 | -1e+09 | -1e+09 |\t|  5 | -6.17685 | -5.20399 | -3.98134 | -3.15001 | -2.66538 | -2.1262  | -2.17109 | -1.94585  | -1e+09 |\n",
      "|  6 | -6.32054 | -5.07264 | -3.77958 | -2.97934 | -2.36116 | -1.8404  | -2.05384 | -1e+09 | -1e+09 |\t|  6 | -8.18793 | -7.05491 | -5.56985 | -4.25333 | -3.62716 | -2.64248 | -2.09466 | -1.21432  | -1e+09 |\n",
      "|  7 | -8.18793 | -6.77286 | -5.2409  | -3.85325 | -3.13962 | -2.01007 | -1.38935 | -1e+09 | -1e+09 |\t|  7 | -1e+09   | -1e+09   | -1e+09   | -1e+09   | -1e+09   | -1e+09   | -1e+09   | -0.605949 | -1e+09 |\n",
      "|  8 | -1e+09   | -1e+09   | -1e+09   | -1e+09   | -1e+09   | -1e+09   | -1e+09   | -1e+09 | -1e+09 |\t|  8 | -1e+09   | -1e+09   | -1e+09   | -1e+09   | -1e+09   | -1e+09   | -1e+09   | -1e+09    | -1e+09 |\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(backward_prob_sub[:x_length+2, :y_length+2])\n",
    "df2 = pd.DataFrame(backward_prob_dlt[:x_length+2, :y_length+2])\n",
    "\n",
    "display_horizontaly_two_tables(df, df2, \"Subsitution Operation\", \"Deletion Operation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rendering of the target and the logits before the loss computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Thomas\\OneDrive\\UNIVERSITÉ\\MATIÈRES\\Projet\\IA_languesAnciennes\\Code\\Unsupervised_reconstruction\\source\\utils.py:65: UserWarning: An output with one or more elements was resized since it had shape [17, 1, 3213, 1], which does not match the required output shape [17, 19, 3213, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\Resize.cpp:35.)\n",
      "  return torch.logical_and(A.unsqueeze(1), B.unsqueeze(0)).to(device)\n",
      "c:\\Users\\Thomas\\OneDrive\\UNIVERSITÉ\\MATIÈRES\\Projet\\IA_languesAnciennes\\Code\\Unsupervised_reconstruction\\source\\utils.py:65: UserWarning: An output with one or more elements was resized since it had shape [17, 1, 3213, 1], which does not match the required output shape [17, 23, 3213, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\Resize.cpp:35.)\n",
      "  return torch.logical_and(A.unsqueeze(1), B.unsqueeze(0)).to(device)\n",
      "c:\\Users\\Thomas\\OneDrive\\UNIVERSITÉ\\MATIÈRES\\Projet\\IA_languesAnciennes\\Code\\Unsupervised_reconstruction\\source\\utils.py:65: UserWarning: An output with one or more elements was resized since it had shape [17, 1, 3213, 1], which does not match the required output shape [17, 25, 3213, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\Resize.cpp:35.)\n",
      "  return torch.logical_and(A.unsqueeze(1), B.unsqueeze(0)).to(device)\n",
      "c:\\Users\\Thomas\\OneDrive\\UNIVERSITÉ\\MATIÈRES\\Projet\\IA_languesAnciennes\\Code\\Unsupervised_reconstruction\\source\\utils.py:65: UserWarning: An output with one or more elements was resized since it had shape [17, 1, 3213, 1], which does not match the required output shape [17, 24, 3213, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\Resize.cpp:35.)\n",
      "  return torch.logical_and(A.unsqueeze(1), B.unsqueeze(0)).to(device)\n"
     ]
    }
   ],
   "source": [
    "for lang in targets_prob.keys():\n",
    "    targets_prob[lang] = targets_prob[lang].toTargetsProbs()\n",
    "targets_prob = [dict(zip(targets_prob,t)) for t in zip(*targets_prob.values())] # list of C dict[ModernLanguages, dict[Operations, Tensor(shape=*)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subsitution Operation                                                                             \tDeletion Operation\n",
      "|    |        0 |        1 |        2 |        3 |        4 |        5 |        6 |      7 |   8 |\t|    |        0 |        1 |        2 |        3 |        4 |        5 |        6 |         7 |   8 |\n",
      "|---:|---------:|---------:|---------:|---------:|---------:|---------:|---------:|-------:|----:|\t|---:|---------:|---------:|---------:|---------:|---------:|---------:|---------:|----------:|----:|\n",
      "|  0 | -1e+09   | -1e+09   | -1e+09   | -1e+09   | -1e+09   | -1e+09   | -1e+09   | -2e+09 |   0 |\t|  0 | -1e+09   | -1e+09   | -1e+09   | -1e+09   | -1e+09   | -1e+09   | -1e+09   | -1e+09    |   0 |\n",
      "|  1 | -1.35371 | -1.86939 | -2.35208 | -3.49419 | -4.72537 | -5.81715 | -7.59627 | -1e+09 |   0 |\t|  1 | -1.80048 | -2.08853 | -2.44592 | -3.08607 | -4.10483 | -4.71093 | -6.2416  | -7.18322  |   0 |\n",
      "|  2 | -2.28378 | -1.84261 | -1.96672 | -2.45948 | -3.33074 | -4.20092 | -5.86989 | -1e+09 |   0 |\t|  2 | -2.75952 | -2.36094 | -2.22304 | -2.35782 | -2.98272 | -3.36245 | -4.58199 | -5.37401  |   0 |\n",
      "|  3 | -3.27249 | -2.31927 | -2.04653 | -2.07499 | -2.58411 | -3.02769 | -4.54346 | -1e+09 |   0 |\t|  3 | -3.67261 | -3.13285 | -2.40697 | -2.24389 | -2.50374 | -2.57778 | -3.4031  | -3.99823  |   0 |\n",
      "|  4 | -4.03044 | -3.1999  | -2.28013 | -2.08759 | -2.27015 | -2.42872 | -3.38564 | -1e+09 |   0 |\t|  4 | -4.87387 | -3.99302 | -3.06672 | -2.47484 | -2.43152 | -2.17711 | -2.58217 | -2.87902  |   0 |\n",
      "|  5 | -5.19094 | -3.94673 | -3.05414 | -2.32763 | -2.25144 | -2.00502 | -2.58155 | -1e+09 |   0 |\t|  5 | -6.17685 | -5.20399 | -3.98134 | -3.15001 | -2.66538 | -2.1262  | -2.17109 | -1.94585  |   0 |\n",
      "|  6 | -6.32054 | -5.07264 | -3.77958 | -2.97934 | -2.36116 | -1.8404  | -2.05384 | -1e+09 |   0 |\t|  6 | -8.18793 | -7.05491 | -5.56985 | -4.25333 | -3.62716 | -2.64248 | -2.09466 | -1.21432  |   0 |\n",
      "|  7 | -8.18793 | -6.77286 | -5.2409  | -3.85325 | -3.13962 | -2.01007 | -1.38935 | -1e+09 |   0 |\t|  7 | -1e+09   | -1e+09   | -1e+09   | -1e+09   | -1e+09   | -1e+09   | -1e+09   | -0.605949 |   0 |\n",
      "|  8 |  0       |  0       |  0       |  0       |  0       |  0       |  0       |  0     |   0 |\t|  8 |  0       |  0       |  0       |  0       |  0       |  0       |  0       |  0        |   0 |\n"
     ]
    }
   ],
   "source": [
    "renderedTargets = targets_prob[IDX][TEST_LANGUAGE]\n",
    "df_renderedSub = pd.DataFrame(renderedTargets['sub'].squeeze(-1).squeeze(-1)[:x_length+2, :y_length+2])\n",
    "df2_renderedDlt = pd.DataFrame(renderedTargets['dlt'].squeeze(-1).squeeze(-1)[:x_length+2, :y_length+2])\n",
    "display_horizontaly_two_tables(df_renderedSub, df2_renderedDlt, \"Subsitution Operation\", \"Deletion Operation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torchdata.datapipes.iter import IterableWrapper\n",
    "dp3 = IterableWrapper(targets_prob)\n",
    "iterator = iter(dp3)\n",
    "firstElt = next(iterator)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdataloader2\u001b[39;00m \u001b[39mimport\u001b[39;00m DataLoader2\n\u001b[1;32m----> 3\u001b[0m dl \u001b[39m=\u001b[39m DataLoader2(dp3)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torchdata\\dataloader2\\dataloader2.py:170\u001b[0m, in \u001b[0;36mDataLoader2.__init__\u001b[1;34m(self, datapipe, datapipe_adapter_fn, reading_service)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(datapipe, MapDataPipe):\n\u001b[0;32m    169\u001b[0m     datapipe \u001b[39m=\u001b[39m datapipe\u001b[39m.\u001b[39mto_iter_datapipe()\n\u001b[1;32m--> 170\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdatapipe \u001b[39m=\u001b[39m clone(datapipe) \u001b[39mif\u001b[39;00m datapipe \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_adapted: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_datapipe_iter: Optional[Iterator[T_co]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torchdata\\dataloader2\\graph\\_serialization.py:86\u001b[0m, in \u001b[0;36mclone\u001b[1;34m(obj)\u001b[0m\n\u001b[0;32m     84\u001b[0m use_dill \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m     85\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 86\u001b[0m     states \u001b[39m=\u001b[39m pickle\u001b[39m.\u001b[39;49mdumps(obj)\n\u001b[0;32m     87\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m     88\u001b[0m     \u001b[39mif\u001b[39;00m HAS_DILL:\n",
      "File \u001b[1;32mc:\\Users\\Thomas\\.pyenv\\pyenv-win\\versions\\3.9.10\\lib\\site-packages\\torch\\_tensor.py:212\u001b[0m, in \u001b[0;36mTensor.__reduce_ex__\u001b[1;34m(self, proto)\u001b[0m\n\u001b[0;32m    209\u001b[0m         memo[\u001b[39mid\u001b[39m(\u001b[39mself\u001b[39m)] \u001b[39m=\u001b[39m new_tensor\n\u001b[0;32m    210\u001b[0m         \u001b[39mreturn\u001b[39;00m new_tensor\n\u001b[1;32m--> 212\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__reduce_ex__\u001b[39m(\u001b[39mself\u001b[39m, proto):\n\u001b[0;32m    213\u001b[0m     state \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_get_obj_state(\u001b[39mself\u001b[39m)\n\u001b[0;32m    214\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39mis\u001b[39;00m Tensor \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m state:\n\u001b[0;32m    215\u001b[0m         \u001b[39m# Fast path for regular tensor without Python state.\u001b[39;00m\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:1457\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:1758\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.ThreadTracer.__call__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Thomas\\.pyenv\\pyenv-win\\versions\\3.9.10\\lib\\site-packages\\debugpy\\_vendored\\pydevd\\_pydev_bundle\\pydev_is_thread_alive.py:10\u001b[0m, in \u001b[0;36mis_thread_alive\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mis_thread_alive\u001b[39m(t):\n\u001b[1;32m---> 10\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mnot\u001b[39;00m t\u001b[39m.\u001b[39;49m_is_stopped\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torchdata.dataloader2 import DataLoader2\n",
    "\n",
    "dl = DataLoader2(dp3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "MINI_BATCH_SIZE = 30\n",
    "from data.datapipes import get_training_datapipe\n",
    "\n",
    "_raw_cognates_list: list[dict[ModernLanguages, str]] = [{lang:raw_cognates[lang][i] for lang in MODERN_LANGUAGES} for i in range(len(raw_cognates[\"french\"]))]\n",
    "_dp2 = IterableWrapper(_raw_cognates_list)\n",
    "_dp1 = IterableWrapper(raw_samples)\n",
    "dp = _dp1.zip(_dp2, _dp3)\n",
    "dp = get_training_datapipe(dp, MINI_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchdata.dataloader2 import DataLoader2\n",
    "\n",
    "dl = DataLoader2(dataset=dp) # issue with DataLoader2 and to_graph in cause of tensors in targets_probs (_dp3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxModernSequenceLength_inMiniBatch: dict[ModernLanguages, int] = {lang:data[2] for (lang, data) in miniBatchDataForLogitsComputation[1].items()} #type:ignore\n",
    "maxSampleSequenceLength_inMiniBatch: int = miniBatchDataForLogitsComputation[0][2]+1\n",
    "renderedTargets = renderedTargets.split([maxModernSequenceLength_inMiniBatch[lang]*maxSampleSequenceLength_inMiniBatch for lang in randomEditModel.languages])[randomEditModel.languages.index(TEST_LANGUAGE)].view(maxSampleSequenceLength_inMiniBatch, maxModernSequenceLength_inMiniBatch[TEST_LANGUAGE], IPA_charsNumber*2+2).cpu().numpy()\n",
    "renderedLogits = renderedLogits.split([maxModernSequenceLength_inMiniBatch[lang]*maxSampleSequenceLength_inMiniBatch for lang in randomEditModel.languages])[randomEditModel.languages.index(TEST_LANGUAGE)].view(maxSampleSequenceLength_inMiniBatch, maxModernSequenceLength_inMiniBatch[TEST_LANGUAGE], IPA_charsNumber*2+2).cpu().numpy()\n",
    "\n",
    "print(renderedTargets.shape)\n",
    "print(renderedLogits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison of the rendered target probs format and the cached probs format\n",
    "\n",
    "A reduction of the renderedTargets has been done for the displaying. The comparison is done for the insertion and the ending operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_insBackward = pd.DataFrame(backward_prob_ins[:x_length+2, :y_length+2])\n",
    "df_endBackward = pd.DataFrame(backward_prob_end[:x_length+2, :y_length+2])\n",
    "df_insRenderedTargets = pd.DataFrame(renderedTargets[:x_length+2, :y_length+2, -IPA_charsNumber-1:-1].sum(axis=2))\n",
    "df_endRenderedTargets = pd.DataFrame(renderedTargets[:x_length+2, :y_length+2, -1])\n",
    "df_insRenderedLogits = pd.DataFrame(renderedLogits[:x_length+2, :y_length+2, -IPA_charsNumber-1:-1].sum(axis=2))\n",
    "df_endRenderedLogits = pd.DataFrame(renderedLogits[:x_length+2, :y_length+2, -1])\n",
    "\n",
    "display_horizontaly_two_tables(df_insBackward, df_endBackward, \"Insertion operation - cached posterior probs\", \"Ending operation - cached posterior probs\")\n",
    "display_horizontaly_two_tables(df_insRenderedTargets, df_endRenderedTargets, \"Insertion operation - rendered target probs\", \"Ending operation - rendered target probs\")\n",
    "display_horizontaly_two_tables(df_insRenderedLogits, df_endRenderedLogits, \"Insertion operation - rendered logits\", \"Ending operation - rendered logits\")\n",
    "\n",
    "# display(pd.merge(df_insBackward, df_endBackward, left_index=True, right_index=True, suffixes=(' ins', ' end')),\n",
    "#         pd.merge(df_insRenderedTargets, df_endRenderedTargets, left_index=True, right_index=True, suffixes=(' ins', ' end'))\n",
    "#         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m EPOCHS, LEARNING_RATE \u001b[39m=\u001b[39m \u001b[39m5\u001b[39m, \u001b[39m0.01\u001b[39m\n\u001b[1;32m----> 2\u001b[0m randomEditModel\u001b[39m.\u001b[39;49mtrain_models(dl, EPOCHS, LEARNING_RATE)\n",
      "File \u001b[1;32mc:\\Users\\Thomas\\OneDrive\\UNIVERSITÉ\\MATIÈRES\\Projet\\IA_languesAnciennes\\Code\\Unsupervised_reconstruction\\source\\reconstructionModel.py:113\u001b[0m, in \u001b[0;36mReconstructionModel.train_models\u001b[1;34m(self, training_data_loader, epochs, learning_rate)\u001b[0m\n\u001b[0;32m    110\u001b[0m training_data_loader\u001b[39m.\u001b[39mseed(epochNumber)\n\u001b[0;32m    111\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepochNumber\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m-\u001b[39m\u001b[39m'\u001b[39m\u001b[39m*\u001b[39m\u001b[39m60\u001b[39m)\n\u001b[1;32m--> 113\u001b[0m losses \u001b[39m=\u001b[39m []\n\u001b[0;32m    114\u001b[0m \u001b[39mfor\u001b[39;00m (i, (sources, cognates, targets_load)) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(training_data_loader):\n\u001b[0;32m    115\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\Thomas\\.pyenv\\pyenv-win\\versions\\3.9.10\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:438\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    436\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterator\n\u001b[0;32m    437\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 438\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_iterator()\n",
      "File \u001b[1;32mc:\\Users\\Thomas\\.pyenv\\pyenv-win\\versions\\3.9.10\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:386\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    384\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    385\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_worker_number_rationality()\n\u001b[1;32m--> 386\u001b[0m     \u001b[39mreturn\u001b[39;00m _MultiProcessingDataLoaderIter(\u001b[39mself\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\Thomas\\.pyenv\\pyenv-win\\versions\\3.9.10\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:991\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[1;34m(self, loader)\u001b[0m\n\u001b[0;32m    990\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, loader):\n\u001b[1;32m--> 991\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(loader)\n\u001b[0;32m    993\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prefetch_factor \u001b[39m=\u001b[39m loader\u001b[39m.\u001b[39mprefetch_factor\n\u001b[0;32m    995\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_workers \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Thomas\\.pyenv\\pyenv-win\\versions\\3.9.10\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:574\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__init__\u001b[1;34m(self, loader)\u001b[0m\n\u001b[0;32m    572\u001b[0m     shared_rng \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mGenerator()\n\u001b[0;32m    573\u001b[0m     shared_rng\u001b[39m.\u001b[39mmanual_seed(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shared_seed)\n\u001b[1;32m--> 574\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mutils\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mgraph_settings\u001b[39m.\u001b[39;49mapply_random_seed(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset, shared_rng)\n\u001b[0;32m    575\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m=\u001b[39m loader\u001b[39m.\u001b[39m_dataset_kind\n\u001b[0;32m    576\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39m=\u001b[39m loader\u001b[39m.\u001b[39m_IterableDataset_len_called\n",
      "File \u001b[1;32mc:\\Users\\Thomas\\.pyenv\\pyenv-win\\versions\\3.9.10\\lib\\site-packages\\torch\\utils\\data\\graph_settings.py:139\u001b[0m, in \u001b[0;36mapply_random_seed\u001b[1;34m(datapipe, rng)\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_random_seed\u001b[39m(datapipe: DataPipe, rng: torch\u001b[39m.\u001b[39mGenerator) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataPipe:\n\u001b[0;32m    131\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[39m    Traverse the graph of ``DataPipes`` to find random ``DataPipe`` with an API of\u001b[39;00m\n\u001b[0;32m    133\u001b[0m \u001b[39m    ``set_seed`` then set the random seed based on the provided RNG.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[39m        rng: Random number generator to generate random seeds\u001b[39;00m\n\u001b[0;32m    138\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 139\u001b[0m     graph \u001b[39m=\u001b[39m traverse_dps(datapipe)\n\u001b[0;32m    140\u001b[0m     all_pipes \u001b[39m=\u001b[39m get_all_graph_pipes(graph)\n\u001b[0;32m    141\u001b[0m     \u001b[39m# Using a set to track id of DataPipe to prevent setting randomness per DataPipe more than once.\u001b[39;00m\n\u001b[0;32m    142\u001b[0m     \u001b[39m# And, `id` is used in case of unhashable DataPipe\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Thomas\\.pyenv\\pyenv-win\\versions\\3.9.10\\lib\\site-packages\\torch\\utils\\data\\graph.py:98\u001b[0m, in \u001b[0;36mtraverse_dps\u001b[1;34m(datapipe)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     86\u001b[0m \u001b[39mTraverse the DataPipes and their attributes to extract the DataPipe graph.\u001b[39;00m\n\u001b[0;32m     87\u001b[0m \u001b[39mThis only looks into the attribute from each DataPipe that is either a\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[39m    and values are tuples of DataPipe instance and the sub-graph\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     97\u001b[0m cache: Set[\u001b[39mint\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n\u001b[1;32m---> 98\u001b[0m \u001b[39mreturn\u001b[39;00m _traverse_helper(datapipe, only_datapipe\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, cache\u001b[39m=\u001b[39;49mcache)\n",
      "File \u001b[1;32mc:\\Users\\Thomas\\.pyenv\\pyenv-win\\versions\\3.9.10\\lib\\site-packages\\torch\\utils\\data\\graph.py:145\u001b[0m, in \u001b[0;36m_traverse_helper\u001b[1;34m(datapipe, only_datapipe, cache)\u001b[0m\n\u001b[0;32m    141\u001b[0m d: DataPipeGraph \u001b[39m=\u001b[39m {dp_id: (datapipe, {})}\n\u001b[0;32m    142\u001b[0m \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m items:\n\u001b[0;32m    143\u001b[0m     \u001b[39m# Using cache.copy() here is to prevent recursion on a single path rather than global graph\u001b[39;00m\n\u001b[0;32m    144\u001b[0m     \u001b[39m# Single DataPipe can present multiple times in different paths in graph\u001b[39;00m\n\u001b[1;32m--> 145\u001b[0m     d[dp_id][\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mupdate(_traverse_helper(item, only_datapipe, cache\u001b[39m.\u001b[39;49mcopy()))\n\u001b[0;32m    146\u001b[0m \u001b[39mreturn\u001b[39;00m d\n",
      "File \u001b[1;32mc:\\Users\\Thomas\\.pyenv\\pyenv-win\\versions\\3.9.10\\lib\\site-packages\\torch\\utils\\data\\graph.py:145\u001b[0m, in \u001b[0;36m_traverse_helper\u001b[1;34m(datapipe, only_datapipe, cache)\u001b[0m\n\u001b[0;32m    141\u001b[0m d: DataPipeGraph \u001b[39m=\u001b[39m {dp_id: (datapipe, {})}\n\u001b[0;32m    142\u001b[0m \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m items:\n\u001b[0;32m    143\u001b[0m     \u001b[39m# Using cache.copy() here is to prevent recursion on a single path rather than global graph\u001b[39;00m\n\u001b[0;32m    144\u001b[0m     \u001b[39m# Single DataPipe can present multiple times in different paths in graph\u001b[39;00m\n\u001b[1;32m--> 145\u001b[0m     d[dp_id][\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mupdate(_traverse_helper(item, only_datapipe, cache\u001b[39m.\u001b[39;49mcopy()))\n\u001b[0;32m    146\u001b[0m \u001b[39mreturn\u001b[39;00m d\n",
      "    \u001b[1;31m[... skipping similar frames: _traverse_helper at line 145 (3 times)]\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Thomas\\.pyenv\\pyenv-win\\versions\\3.9.10\\lib\\site-packages\\torch\\utils\\data\\graph.py:145\u001b[0m, in \u001b[0;36m_traverse_helper\u001b[1;34m(datapipe, only_datapipe, cache)\u001b[0m\n\u001b[0;32m    141\u001b[0m d: DataPipeGraph \u001b[39m=\u001b[39m {dp_id: (datapipe, {})}\n\u001b[0;32m    142\u001b[0m \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m items:\n\u001b[0;32m    143\u001b[0m     \u001b[39m# Using cache.copy() here is to prevent recursion on a single path rather than global graph\u001b[39;00m\n\u001b[0;32m    144\u001b[0m     \u001b[39m# Single DataPipe can present multiple times in different paths in graph\u001b[39;00m\n\u001b[1;32m--> 145\u001b[0m     d[dp_id][\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mupdate(_traverse_helper(item, only_datapipe, cache\u001b[39m.\u001b[39;49mcopy()))\n\u001b[0;32m    146\u001b[0m \u001b[39mreturn\u001b[39;00m d\n",
      "File \u001b[1;32mc:\\Users\\Thomas\\.pyenv\\pyenv-win\\versions\\3.9.10\\lib\\site-packages\\torch\\utils\\data\\graph.py:140\u001b[0m, in \u001b[0;36m_traverse_helper\u001b[1;34m(datapipe, only_datapipe, cache)\u001b[0m\n\u001b[0;32m    138\u001b[0m cache\u001b[39m.\u001b[39madd(dp_id)\n\u001b[0;32m    139\u001b[0m \u001b[39m# Using cache.copy() here is to prevent the same DataPipe pollutes the cache on different paths\u001b[39;00m\n\u001b[1;32m--> 140\u001b[0m items \u001b[39m=\u001b[39m _list_connected_datapipes(datapipe, only_datapipe, cache\u001b[39m.\u001b[39;49mcopy())\n\u001b[0;32m    141\u001b[0m d: DataPipeGraph \u001b[39m=\u001b[39m {dp_id: (datapipe, {})}\n\u001b[0;32m    142\u001b[0m \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m items:\n\u001b[0;32m    143\u001b[0m     \u001b[39m# Using cache.copy() here is to prevent recursion on a single path rather than global graph\u001b[39;00m\n\u001b[0;32m    144\u001b[0m     \u001b[39m# Single DataPipe can present multiple times in different paths in graph\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Thomas\\.pyenv\\pyenv-win\\versions\\3.9.10\\lib\\site-packages\\torch\\utils\\data\\graph.py:67\u001b[0m, in \u001b[0;36m_list_connected_datapipes\u001b[1;34m(scan_obj, only_datapipe, cache)\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mset_getstate_hook(getstate_hook)\n\u001b[0;32m     66\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 67\u001b[0m     p\u001b[39m.\u001b[39;49mdump(scan_obj)\n\u001b[0;32m     68\u001b[0m \u001b[39mexcept\u001b[39;00m (pickle\u001b[39m.\u001b[39mPickleError, \u001b[39mAttributeError\u001b[39;00m, \u001b[39mTypeError\u001b[39;00m):\n\u001b[0;32m     69\u001b[0m     \u001b[39mif\u001b[39;00m DILL_AVAILABLE:\n",
      "File \u001b[1;32mc:\\Users\\Thomas\\.pyenv\\pyenv-win\\versions\\3.9.10\\lib\\site-packages\\torch\\_tensor.py:213\u001b[0m, in \u001b[0;36mTensor.__reduce_ex__\u001b[1;34m(self, proto)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__reduce_ex__\u001b[39m(\u001b[39mself\u001b[39m, proto):\n\u001b[1;32m--> 213\u001b[0m     state \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_get_obj_state(\u001b[39mself\u001b[39m)\n\u001b[0;32m    214\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39mis\u001b[39;00m Tensor \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m state:\n\u001b[0;32m    215\u001b[0m         \u001b[39m# Fast path for regular tensor without Python state.\u001b[39;00m\n\u001b[0;32m    216\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reduce_ex_internal(proto)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EPOCHS, LEARNING_RATE = 5, 0.01\n",
    "randomEditModel.train_models(dl, EPOCHS, LEARNING_RATE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
