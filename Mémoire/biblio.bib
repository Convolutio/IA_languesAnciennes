@inbook{jurafsky_ffnn,
  author         = {Dan Jurafsky, James H. Martin},
  chapter        = {7},
  booktitle      = {Speech and Language Processing},
  pages          = {},
  url            = {https://web.stanford.edu/~jurafsky/slp3/7},
  institution    = {Stanford University},
  title          = {Neural Networks and Neural Language Models},
  year           = {2023}
}

@inbook{jurafsky_rnn-lstm,
  author         = {Dan Jurafsky, James H. Martin},
  chapter        = {9},
  booktitle      = {Speech and Language Processing},
  pages          = {},
  url            = {https://web.stanford.edu/~jurafsky/slp3/7},
  institution    = {Stanford University},
  title          = {RNNs and LSTMs},
}

@inbook{jurafsky_vector,
  author         = {Dan Jurafsky, James H. Martin},
  chapter        = {6},
  booktitle      = {Speech and Language Processing},
  pages          = {},
  url            = {https://web.stanford.edu/~jurafsky/slp3/6},
  institution    = {Stanford University},
  title          = {Vector Semantics and Embeddings},
  year           = {2023}
}

@inbook{jurafsky_regular,
  author         = {Dan Jurafsky, James H. Martin},
  chapter        = {2},
  booktitle      = {Speech and Language Processing},
  pages          = {},
  url            = {https://web.stanford.edu/~jurafsky/slp3/2},
  institution    = {Stanford University},
  title          = {Regular Expressions, Text Normalization, Edit Distance},
  year           = {2023}
}

@phdthesis{fourrier,
  TITLE = {{Neural Approaches to Historical Word Reconstruction}},
  AUTHOR = {Fourrier, Cl{\'e}mentine},
  URL = {https://hal.inria.fr/tel-03793299},
  SCHOOL = {{Universit{\'e} PSL (Paris Sciences \& Lettres)}},
  YEAR = {2022},
  MONTH = Sep,
  KEYWORDS = {Natural Language Processing (NLP) ; Computational linguistics ; Computational etymology ; Historical words reconstruction ; Cognates ; Neural Machine Translation (NMT) ; Traitement Automatique des Langues (TAL) ; Linguistique computationnelle ; Etymologie computationelle ; Re-construction de mots historiques ; Cognats ; Traduction automatique neuronale},
  TYPE = {Theses},
  PDF = {https://hal.inria.fr/tel-03793299v2/file/thesis.pdf},
  HAL_ID = {tel-03793299},
  HAL_VERSION = {v2},
}

@proceedings{bnf_etrusque,
  editor          = {},
  organization    = {BNF},
  series          = {},
  title           = {L'écriture et la langue étrusques : histoire d'un déchiffrement et d'une conquête scientifique en cours},
  url             = {https://www.bnf.fr/fr/agenda/lecriture-et-la-langue-etrusques-histoire-dun-dechiffrement-et-dune-conquete-scientifique-en},
  year            = {2022},
  month           = {3},
}

@inproceedings{meloni-etal-2021-ab,
    title = {Ab Antiquo: Neural Proto-language Reconstruction},
    author = {Meloni, Carlo and Ravfogel, Shauli and Goldberg, Yoav},
    booktitle = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
    month = jun,
    year = {2021},
    address = {Online},
    publisher = {Association for Computational Linguistics},
    url = {https://aclanthology.org/2021.naacl-main.353},
    doi = {10.18653/v1/2021.naacl-main.353},
    pages = {4460--4473},
    abstract = {Historical linguists have identified regularities in the process of historic sound change. The comparative method utilizes those regularities to reconstruct proto-words based on observed forms in daughter languages. Can this process be efficiently automated? We address the task of proto-word reconstruction, in which the model is exposed to cognates in contemporary daughter languages, and has to predict the proto word in the ancestor language. We provide a novel dataset for this task, encompassing over 8,000 comparative entries, and show that neural sequence models outperform conventional methods applied to this task so far. Error analysis reveals a variability in the ability of neural model to capture different phonological changes, correlating with the complexity of the changes. Analysis of learned embeddings reveals the models learn phonologically meaningful generalizations, corresponding to well-attested phonological shifts documented by historical linguistics.},
}

@misc{transformer,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2017},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{he2022neural,
      title={Neural Unsupervised Reconstruction of Protolanguage Word Forms}, 
      author={Andre He and Nicholas Tomlin and Dan Klein},
      year={2022},
      eprint={2211.08684},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{kingma2017adam,
      title={Adam: A Method for Stochastic Optimization}, 
      author={Diederik P. Kingma and Jimmy Ba},
      year={2017},
      eprint={1412.6980},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{deepmind2022,
  Title = {Restoring and attributing ancient texts using deep neural networks},
	Author = {Assael, Yannis and Sommerschield, Thea and Shillingford, Brendan and Bordbar, Mahyar and Pavlopoulos, John and Chatzipanagiotou, Marita and Androutsopoulos, Ion and Prag, Jonathan and de Freitas, Nando},
	DOI = {10.1038/s41586-022-04448-z},
	Number = {7900},
	Volume = {603},
	Month = {March},
	Year = {2022},
	Journal = {Nature},
	ISSN = {0028-0836},
	Pages = {280—283},
  URL = {https://europepmc.org/articles/PMC8907065},
} 
  
  
 @article{luo-et-al-2020
  author= {Jiaming Luo and
                  Frederik Hartmann and
                  Enrico Santus and
                  Yuan Cao and
                  Regina Barzilay},
  title= {Deciphering Undersegmented Ancient Scripts Using Phonetic Prior},
  journal= {CoRR},
  volume= {abs/2010.11054},
  year= {2020},
  url= {https://arxiv.org/abs/2010.11054},
  eprinttype = {arXiv},
  eprint= {2010.11054},
  timestamp= {Mon, 26 Oct 2020 15:39:44 +0100},
  biburl= {https://dblp.org/rec/journals/corr/abs-2010-11054.bib},
  bibsource= {dblp computer science bibliography, https://dblp.org}
}

@article{linear-elamite-writing, 
url = {https://doi.org/10.1515/za-2022-0003},
title = {The Decipherment of Linear Elamite Writing},
author = {François Desset and Kambiz Tabibzadeh and Matthieu Kervran and Gian Pietro Basello and and Gianni Marchesi},
pages = {11--60},
volume = {112},
number = {1},
journal = {Zeitschrift für Assyriologie und vorderasiatische Archäologie},
doi = {doi:10.1515/za-2022-0003},
year = {2022},
} 

@article{ugaritic-and-linear-B
author       = {Jiaming Luo and
                  Yuan Cao and
                  Regina Barzilay},
  title        = {Neural Decipherment via Minimum-Cost Flow: from Ugaritic to Linear
                  {B}},
  journal      = {CoRR},
  volume       = {abs/1906.06718},
  year         = {2019},
  url          = {http://arxiv.org/abs/1906.06718},
  eprinttype    = {arXiv},
  eprint       = {1906.06718},
  timestamp    = {Mon, 24 Jun 2019 17:28:45 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1906-06718.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{mikolov2013efficient,
      title={Efficient Estimation of Word Representations in Vector Space}, 
      author={Tomas Mikolov and Kai Chen and Greg Corrado and Jeffrey Dean},
      year={2013},
      eprint={1301.3781},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{mikolov2013distributed,
      title={Distributed Representations of Words and Phrases and their Compositionality}, 
      author={Tomas Mikolov and Ilya Sutskever and Kai Chen and Greg Corrado and Jeffrey Dean},
      year={2013},
      eprint={1310.4546},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{glove,
  author = {Jeffrey Pennington and Richard Socher and Christopher D. Manning},
  booktitle = {Empirical Methods in Natural Language Processing (EMNLP)},
  title = {GloVe: Global Vectors for Word Representation},
  year = {2014},
  pages = {1532--1543},
  url = {http://www.aclweb.org/anthology/D14-1162},
}

@misc{bert,
      title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}, 
      author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
      year={2019},
      eprint={1810.04805},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{bpe,
    title = {Neural Machine Translation of Rare Words with Subword Units},
    author = {Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
    booktitle = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
    month = aug,
    year = {2016},
    address = {Berlin, Germany},
    publisher = {Association for Computational Linguistics},
    url = {https://aclanthology.org/P16-1162},
    doi = {10.18653/v1/P16-1162},
    pages = {1715--1725},
}

@inproceedings{unigramkudo2018,
    title = {Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates},
    author = {Kudo, Taku},
    booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
    month = jul,
    year = {2018},
    address = {Melbourne, Australia},
    publisher = {Association for Computational Linguistics},
    url = {https://aclanthology.org/P18-1007},
    doi = {10.18653/v1/P18-1007},
    pages = {66--75},
    abstract = {Subword units are an effective way to alleviate the open vocabulary problems in neural machine translation (NMT). While sentences are usually converted into unique subword sequences, subword segmentation is potentially ambiguous and multiple segmentations are possible even with the same vocabulary. The question addressed in this paper is whether it is possible to harness the segmentation ambiguity as a noise to improve the robustness of NMT. We present a simple regularization method, subword regularization, which trains the model with multiple subword segmentations probabilistically sampled during training. In addition, for better subword sampling, we propose a new subword segmentation algorithm based on a unigram language model. We experiment with multiple corpora and report consistent improvements especially on low resource and out-of-domain settings.},
}

@inproceedings{wordpiece,
  author={Schuster, Mike and Nakajima, Kaisuke},
  booktitle={2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Japanese and Korean voice search}, 
  year={2012},
  volume={},
  number={},
  pages={5149-5152},
  doi={10.1109/ICASSP.2012.6289079}
}

@inproceedings{sentencepiece,
    title = {SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing},
    author = {Kudo, Taku and Richardson, John},
    booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations},
    month = nov,
    year = {2018},
    address = {Brussels, Belgium},
    publisher = {Association for Computational Linguistics},
    url = {https://aclanthology.org/D18-2012},
    doi = {10.18653/v1/D18-2012},
    pages = {66--71},
    abstract = {This paper describes SentencePiece, a language-independent subword tokenizer and detokenizer designed for Neural-based text processing, including Neural Machine Translation. It provides open-source C++ and Python implementations for subword units. While existing subword segmentation tools assume that the input is pre-tokenized into word sequences, SentencePiece can train subword models directly from raw sentences, which allows us to make a purely end-to-end and language independent system. We perform a validation experiment of NMT on English-Japanese machine translation, and find that it is possible to achieve comparable accuracy to direct subword training from raw sentences. We also compare the performance of subword training and segmentation with various configurations. SentencePiece is available under the Apache 2 license at \url{https://github.com/google/sentencepiece}.},
}
