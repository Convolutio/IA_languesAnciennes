{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prior Language Models\n",
    "\n",
    "*Computing of $p(x)$*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this before continuing so that the imports work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import chdir, getcwd\n",
    "\n",
    "if getcwd().endswith('notebooks'):\n",
    "    chdir('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|IPA| = 57\n",
      "( 57\n",
      ") 58\n",
      "- 59\n"
     ]
    }
   ],
   "source": [
    "from data.vocab import vocabulary\n",
    "from models.models import EOS_TOKEN, SOS_TOKEN, PADDING_TOKEN\n",
    "\n",
    "print('|IPA| =', len(vocabulary)-3) # '-3' because `vocabulary` contains the IPA characters plus the special tokens listed below\n",
    "for token in (SOS_TOKEN, EOS_TOKEN, PADDING_TOKEN):\n",
    "    print(token, vocabulary[token])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.getDataset import getLMTrainingSet\n",
    "\n",
    "# Following 'Article Scientifique' there are three db of different sizes.\n",
    "DB_SIZE = [20_000, 10_000, 5_000]\n",
    "\n",
    "# Generate the three db of different sizes.\n",
    "tokens_20k, tokens_10k, tokens_5k = getLMTrainingSet(DB_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x1fdd7b32dc0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data.backward_compatibility import worker_init_fn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.datasets import CC100\n",
    "\n",
    "dp = CC100(root='./out/cache', language_code='la')\n",
    "DataLoader(dp, shuffle=True, num_workers=4, worker_init_fn=worker_init_fn, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57,\n",
      "         57, 57, 57, 57, 57, 57],\n",
      "        [32,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0],\n",
      "        [ 1,  1,  1,  1, 55,  1,  1,  1,  1,  1,  1,  1, 43, 43,  1,  1,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1],\n",
      "        [ 0,  0,  2, 32,  7,  9,  9,  9, 11, 17, 17, 30, 15, 15, 15, 15, 15, 15,\n",
      "         15, 15, 15, 30, 30, 21],\n",
      "        [11,  2,  6, 14, 32,  0,  0, 43, 36,  9, 10, 14, 37, 32, 37, 30, 16, 16,\n",
      "         16, 16, 21, 11, 11, 15],\n",
      "        [43, 32,  8,  0,  8, 56, 16, 56, 33,  6,  6, 16, 11, 11, 11, 14,  6,  6,\n",
      "         14, 14, 14,  2,  2, 43],\n",
      "        [58, 15,  0, 56, 16, 27,  6, 27,  0,  3, 11,  6, 16, 15, 16,  1, 11, 11,\n",
      "          0,  0,  2,  0,  0, 58],\n",
      "        [59,  0, 14, 15, 43, 30, 18, 30, 15, 58,  0, 18, 58,  0, 43, 14, 32, 37,\n",
      "          8,  8, 43, 14, 11, 59],\n",
      "        [59, 58, 58, 30, 58, 58, 43, 58, 30, 59,  1, 43, 59, 58, 58,  3, 11, 11,\n",
      "         15, 15, 58, 58, 15, 59],\n",
      "        [59, 59, 59, 58, 59, 59, 58, 59, 58, 59,  9, 58, 59, 59, 59, 58, 16, 15,\n",
      "         30, 30, 59, 59,  0, 59],\n",
      "        [59, 59, 59, 59, 59, 59, 59, 59, 59, 59,  9, 59, 59, 59, 59, 59, 58,  0,\n",
      "         30, 58, 59, 59, 58, 59],\n",
      "        [59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 58, 59, 59, 59, 59, 59, 59, 58,\n",
      "         58, 59, 59, 59, 59, 59]], device='cuda:0', dtype=torch.int32), tensor([ 5,  7,  7,  8,  7,  7,  8,  7,  8,  6, 10,  8,  6,  7,  7,  8,  9, 10,\n",
      "        10,  9,  7,  7,  9,  5]), 10)\n"
     ]
    }
   ],
   "source": [
    "from data.getDataset import getIteration\n",
    "from data.vocab import computeInferenceData\n",
    "\n",
    "sources = computeInferenceData(getIteration(3)[:, :24], vocabulary)\n",
    "print(sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word for test: (absyrdʊ)---\n",
      "\n",
      "word IntTensor: tensor([57,  0,  1, 15, 21, 14,  2, 43, 58, 59, 59, 59], device='cuda:0',\n",
      "       dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "from data.vocab import oneHotsToWords\n",
    "\n",
    "testIndexInBatch = 20\n",
    "word = oneHotsToWords(sources[0][:,testIndexInBatch:testIndexInBatch+1], False, vocabulary)[0]\n",
    "print(f\"word for test: {word}\\n\")\n",
    "print(f\"word IntTensor: {sources[0][:, testIndexInBatch]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN LM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\screamnox\\Desktop\\School\\Projet CPBx\\Github\\IA_languesAnciennes\\Code\\Unsupervised_reconstruction\\.env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torch.cuda import is_available\n",
    "from lm.PriorLM import CharLM\n",
    "\n",
    "device = 'cuda' if is_available() else 'cpu'\n",
    "\n",
    "# Init the character level LSTM language model \n",
    "LSTM_lm = CharLM(embedding_size=1024, hidden_size=100, num_layers=2, dropout_rate=0.1, vocab=vocabulary).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***`TODO`***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p('(absyrdʊ)---') = -28.352100372314453\n"
     ]
    }
   ],
   "source": [
    "probs = LSTM_lm.inference(sources)\n",
    "print(f\"p('{word}') =\", probs[testIndexInBatch].item())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $n$-gram LM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lm.PriorLM import NGramLM\n",
    "\n",
    "bigram_20k = NGramLM(n=2, vocab=vocabulary)\n",
    "bigram_10k = NGramLM(n=2, vocab=vocabulary)\n",
    "bigram_5k = NGramLM(n=2, vocab=vocabulary)\n",
    "\n",
    "trigram_20k = NGramLM(n=3, vocab=vocabulary)\n",
    "trigram_10k = NGramLM(n=3, vocab=vocabulary)\n",
    "trigram_5k = NGramLM(n=3, vocab=vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[57,  0],\n",
      "         [57,  6]],\n",
      "\n",
      "        [[ 0,  1],\n",
      "         [ 6,  4]],\n",
      "\n",
      "        [[ 1, 15],\n",
      "         [ 4,  6]],\n",
      "\n",
      "        [[15, 21],\n",
      "         [ 6,  8]],\n",
      "\n",
      "        [[21, 14],\n",
      "         [ 8,  0]],\n",
      "\n",
      "        [[14,  2],\n",
      "         [ 0, 14]],\n",
      "\n",
      "        [[ 2, 43],\n",
      "         [14,  3]],\n",
      "\n",
      "        [[43, 58],\n",
      "         [ 3, 58]]], device='cuda:0', dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "sentence_test = \"absyrdʊ ifikare\"\n",
    "batch = bigram_20k.batch_ngram(sentence_test)\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.0000e-05,  1.0000e-05,  1.0000e-05,  ...,  1.0000e-05,\n",
       "           1.0000e-05,  1.0000e-05],\n",
       "         [-4.5163e+00,  1.0000e-05, -5.2095e+00,  ...,  1.0000e-05,\n",
       "          -2.8223e-01,  1.0000e-05],\n",
       "         [-2.4849e+00,  1.0000e-05,  1.0000e-05,  ...,  1.0000e-05,\n",
       "          -9.5343e-01,  1.0000e-05],\n",
       "         ...,\n",
       "         [ 1.0000e-05,  1.0000e-05,  1.0000e-05,  ...,  1.0000e-05,\n",
       "           1.0000e-05,  1.0000e-05],\n",
       "         [ 1.0000e-05,  1.0000e-05,  1.0000e-05,  ...,  1.0000e-05,\n",
       "           0.0000e+00,  1.0000e-05],\n",
       "         [ 1.0000e-05,  1.0000e-05,  1.0000e-05,  ...,  1.0000e-05,\n",
       "           1.0000e-05,  1.0000e-05]],\n",
       "\n",
       "        [[ 1.0000e-05, -2.6741e+00,  1.0000e-05,  ...,  1.0000e-05,\n",
       "          -3.3673e+00,  1.0000e-05],\n",
       "         [ 1.0000e-05,  1.0000e-05,  1.0000e-05,  ...,  1.0000e-05,\n",
       "           1.0000e-05,  1.0000e-05],\n",
       "         [ 1.0000e-05,  1.0000e-05,  1.0000e-05,  ...,  1.0000e-05,\n",
       "           1.0000e-05,  1.0000e-05],\n",
       "         ...,\n",
       "         [ 1.0000e-05,  1.0000e-05,  1.0000e-05,  ...,  1.0000e-05,\n",
       "           1.0000e-05,  1.0000e-05],\n",
       "         [ 1.0000e-05,  1.0000e-05,  1.0000e-05,  ...,  1.0000e-05,\n",
       "           0.0000e+00,  1.0000e-05],\n",
       "         [ 1.0000e-05,  1.0000e-05,  1.0000e-05,  ...,  1.0000e-05,\n",
       "           1.0000e-05,  1.0000e-05]],\n",
       "\n",
       "        [[ 1.0000e-05, -4.0431e+00,  1.0000e-05,  ...,  1.0000e-05,\n",
       "          -1.2705e+00,  1.0000e-05],\n",
       "         [ 1.0000e-05,  1.0000e-05,  1.0000e-05,  ...,  1.0000e-05,\n",
       "           1.0000e-05,  1.0000e-05],\n",
       "         [ 1.0000e-05,  1.0000e-05,  1.0000e-05,  ...,  1.0000e-05,\n",
       "           1.0000e-05,  1.0000e-05],\n",
       "         ...,\n",
       "         [ 1.0000e-05,  1.0000e-05,  1.0000e-05,  ...,  1.0000e-05,\n",
       "           1.0000e-05,  1.0000e-05],\n",
       "         [ 1.0000e-05,  1.0000e-05,  1.0000e-05,  ...,  1.0000e-05,\n",
       "           0.0000e+00,  1.0000e-05],\n",
       "         [ 1.0000e-05,  1.0000e-05,  1.0000e-05,  ...,  1.0000e-05,\n",
       "           1.0000e-05,  1.0000e-05]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 1.0000e-05, -7.8016e-01, -1.8112e+00,  ...,  1.0000e-05,\n",
       "          -5.7430e+00,  1.0000e-05],\n",
       "         [-1.5724e+00,  1.0000e-05,  1.0000e-05,  ...,  1.0000e-05,\n",
       "           1.0000e-05,  1.0000e-05],\n",
       "         [-3.3801e+00,  1.0000e-05,  1.0000e-05,  ...,  1.0000e-05,\n",
       "           1.0000e-05,  1.0000e-05],\n",
       "         ...,\n",
       "         [-2.7742e+00, -3.8538e+00, -3.0576e+00,  ...,  1.0000e-05,\n",
       "           1.0000e-05,  1.0000e-05],\n",
       "         [ 1.0000e-05,  1.0000e-05,  1.0000e-05,  ...,  1.0000e-05,\n",
       "           1.0000e-05,  1.0000e-05],\n",
       "         [ 1.0000e-05,  1.0000e-05,  1.0000e-05,  ...,  1.0000e-05,\n",
       "           1.0000e-05,  1.0000e-05]],\n",
       "\n",
       "        [[ 1.0000e-05,  1.0000e-05,  1.0000e-05,  ...,  1.0000e-05,\n",
       "           1.0000e-05,  1.0000e-05],\n",
       "         [ 1.0000e-05,  1.0000e-05,  1.0000e-05,  ...,  1.0000e-05,\n",
       "           1.0000e-05,  1.0000e-05],\n",
       "         [ 1.0000e-05,  1.0000e-05,  1.0000e-05,  ...,  1.0000e-05,\n",
       "           1.0000e-05,  1.0000e-05],\n",
       "         ...,\n",
       "         [ 1.0000e-05,  1.0000e-05,  1.0000e-05,  ...,  1.0000e-05,\n",
       "           1.0000e-05,  1.0000e-05],\n",
       "         [ 1.0000e-05,  1.0000e-05,  1.0000e-05,  ...,  1.0000e-05,\n",
       "           1.0000e-05,  1.0000e-05],\n",
       "         [ 1.0000e-05,  1.0000e-05,  1.0000e-05,  ...,  1.0000e-05,\n",
       "           1.0000e-05,  1.0000e-05]],\n",
       "\n",
       "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00],\n",
       "         ...,\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00]]], device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Torch MP\n",
    "bigram_20k.train(tokens_20k)\n",
    "bigram_10k.train(tokens_10k)\n",
    "bigram_5k.train(tokens_5k)\n",
    "\n",
    "trigram_20k.train(tokens_20k)\n",
    "trigram_10k.train(tokens_10k)\n",
    "trigram_5k.train(tokens_5k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-12.8235, -15.4207, -13.8246, -11.5589,  -9.7588,  -3.5543,  -5.3734,\n",
       "         -3.5543,  -8.4310,  -3.5543,  -6.7731, -12.7137, -20.8093, -15.4825,\n",
       "        -14.3738, -12.9782, -17.1908, -18.7406, -17.7046, -18.8678,  -8.4200,\n",
       "        -18.5905, -25.0964,  -3.5543], device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_20k.inference(sources)\n",
    "bigram_10k.inference(sources)\n",
    "bigram_5k.inference(sources)\n",
    "\n",
    "trigram_20k.inference(sources)\n",
    "trigram_10k.inference(sources)\n",
    "trigram_5k.inference(sources)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
