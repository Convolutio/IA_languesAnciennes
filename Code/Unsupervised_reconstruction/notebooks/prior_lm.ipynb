{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prior Language Models\n",
    "\n",
    "*Computing of $p(x)$*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`config`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import chdir, getcwd\n",
    "\n",
    "if getcwd().endswith('notebooks'):\n",
    "    chdir('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|IPA| = 57\n",
      ") 57\n",
      "( 58\n",
      "- 59\n"
     ]
    }
   ],
   "source": [
    "from data.vocab import vocabulary\n",
    "from models.models import EOS_TOKEN, SOS_TOKEN, PADDING_TOKEN\n",
    "print('|IPA| =', len(vocabulary)-3)\n",
    "for token in (EOS_TOKEN, SOS_TOKEN, PADDING_TOKEN):\n",
    "    print(token, vocabulary[token])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.getDataset import getIteration\n",
    "from data.vocab import computeInferenceData\n",
    "\n",
    "sources = computeInferenceData(getIteration(3)[:, :24], vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word for test = (absyrdʊ)---\n",
      "\n",
      " word IntTensor:\n",
      "tensor([58,  0,  1, 15, 21, 14,  2, 43, 57, 59, 59, 59], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "from data.vocab import oneHotsToWords\n",
    "\n",
    "testIndexInBatch = 20\n",
    "word = oneHotsToWords(sources[0][:,testIndexInBatch:testIndexInBatch+1], False, vocabulary)[0]\n",
    "print(\"word for test =\", word)\n",
    "print(\"\\n word IntTensor:\")\n",
    "print(sources[0][:, testIndexInBatch])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x232a517c700>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data.backward_compatibility import worker_init_fn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.datasets import CC100\n",
    "\n",
    "dp = CC100(root='./out/cache', language_code='la')\n",
    "DataLoader(dp, shuffle=True, num_workers=4, worker_init_fn=worker_init_fn, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN LM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lm.PriorLM import CharLM\n",
    "\n",
    "EMBEDDING_SIZE = 1024\n",
    "HIDDEN_SIZE = 100\n",
    "NUMBER_OF_LAYER = 2\n",
    "lm = CharLM(1024, 100, 2, 0.1, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p('(absyrdʊ)---') = -32.569068908691406\n"
     ]
    }
   ],
   "source": [
    "probs = lm.inference(sources)\n",
    "print(f\"p('{word}') =\", probs[testIndexInBatch].item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***`TODO`***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $n$-gram LM\n",
    "\n",
    "* If $L < n$:\n",
    "\n",
    "If the sequence's length is lower than $n$ -- the order of the $n$-gram --, so the sequence is padded with closing boundaries `')'`.\\\\\n",
    "For instance, `'(b)'` string will be processed by the 4-gram as `'(b))'` so at least one inference can be done in the language model.\n",
    "\n",
    "In training, the probabilities of this kind of $n$-gram will be computed by lower-order $(n-k)$-gram models.\n",
    "\n",
    "* If $L < L_\\textrm{max}$:\n",
    "\n",
    "The inference in a batch of strings will force the algorithm to process $n$-grams containing padding empty characters. To neutralize their useless probabilities, the empty characters are represented with a \"full one-hot\" vector, containing only values equaling `1`. Then, the computed transition probabilities will be higher than 1 and we after limit it to 1 with the max operator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***`TODO`: review this section with new NGram implementation***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pack_sequence\n",
    "from torch.nn.functional import one_hot\n",
    "V = {'a':0, 'b':1, 'c':2, \"(\":3, ')':4}\n",
    "V_inv = ['a', 'b', 'c', \"(\", \")\"]\n",
    "raw_batch = ['(ab)', '(abcb)', '(cba)', '(b)']\n",
    "batch = pack_sequence([one_hot(torch.LongTensor([V[c] for c in w])) for w in raw_batch], enforce_sorted=False) # size = (L, B)\n",
    "\n",
    "from lm.PriorLM import NGramLM\n",
    "\n",
    "model = NGramLM([], 4)\n",
    "paddedData = torch.exp(model.padDataToNgram((batch, None, None)))\n",
    "for w in range(4):\n",
    "    print(f'\\\"{raw_batch[w]}\\\"')\n",
    "    print(paddedData[:,w,:]) # dim = (L, V)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
